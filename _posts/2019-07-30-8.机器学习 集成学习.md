---
layout:     post
title:      8.机器学习|集成学习
subtitle:   机器学习周志华 学习笔记————机器学习|集成学习
date:       2019-07-30
author:     正版慕言
header-img: img/blog_bg_3.jpg
catalog: true
tags:
    - 机器学习
    - 西瓜书
    - 集成学习

---

#### 个体与集成
集成学习通过构建合并多个学习器来完成学习任务，有时也被称为多分类器系统、基于委员会的学习等。
集成学习的一般结构：先产生一组个体学习器，再用某种策略把它们结合起来。
同质集成：集成中只包含同种类别的个体学习器。同质集成中的个体学习器也叫作“基学习器”。
异质集成：集成中包含了不同类型的个体学习器。异质集成中的个体学习器叫做“组件学习器”，或者直接成为个体学习器。
![西瓜书-8.集成学习示意图.png](/img/西瓜书-8.集成学习示意图.png)
集成学习通常能够获得比单一学习器更好的泛化性能，这对弱学习器更加明显。
要获得好的集成，个体学习器应该“好而不同”。个体学习器应该有一定的**准确性**，同时也要有**多样性**。

对于一个二分类问题，假设集成方法使用投票法且基分类器的错误率相互独立，由Hoeffding不等式可知集成错误率：
![西瓜书-8.Hoeffding集成错误率.gif](/img/西瓜书-8.Hoeffding集成错误率.gif)
可以看出，随着分类器数目T增大，集成错误率指数下降，趋于0。

以上分析有一个关键假设：基学习器之间的误差相互独立。一般的，由于这些学习器是针对同一个问题训练的，显然不可能相互独立。在准确性很高之后，想增加多样性就必须牺牲准确性。**如何产生“好而不同”的个体学习器，是集成学习研究的核心。**

目前的集成学习方法可以根据个体学习器的生成方式分为两大类：

* 个体学习器间存在强依赖关系、必须串行生成的序列化方法。代表：Boosting
* 个体学习器间不存在强依赖关系、可同时生成的并行化方法。代表：Bagging和随机森林（Random Forest）

#### Boosting
Boosting是一族能够将弱学习器提升为强学习器的算法。**工作机制：** 先从初始训练集训练出一个基学习器，再根据学习器的表现对训练样本分布进行调整，使先前基学习器做错的训练样本在后续受到更多关注。然后基于调整后的样本分布来训练下一个基学习器。如此重复进行，直到基学习器数目达到事先指定的T。最后将这T个学习器进行加权结合。

**最著名的代表是AdaBoost。**
考虑一个二分类问题，y_i ∈{-1, 1}和真实函数f，AdaBoost算法的描述如下：
![西瓜书-8.AdaBoost算法.png](/img/西瓜书-8.AdaBoost算法.png)

基于“加性模型”
![西瓜书-8.加性模型.gif](/img/西瓜书-8.加性模型.gif) 
来最小化指数损失函数：
![西瓜书-8.加性模型最小化指数损失函数.gif](/img/西瓜书-8.加性模型最小化指数损失函数.gif)

如果H(x)能够令指数损失函数最小化，那么考虑偏导数
![西瓜书-8.指数损失函数求偏导.gif](/img/西瓜书-8.指数损失函数求偏导.gif)
令其=0解得
![西瓜书-8.指数损失函数解偏导.gif](/img/西瓜书-8.指数损失函数解偏导.gif)
有
![西瓜书-8.指数损失函数sign.gif](/img/西瓜书-8.指数损失函数sign.gif)
说明sign(H(**x**))达到了贝叶斯最有错误率。它具有更好的数学性质，因此我们可以用它替代0/1损失函数作为优化目标。

分类器权重更新公式：
![西瓜书-8.AdaBoost分类器权重更新公式.gif](/img/西瓜书-8.AdaBoost分类器权重更新公式.gif)
样本分布更新公式：
![西瓜书-8.AdaBoost分类器样本分布更新公式.gif](/img/西瓜书-8.AdaBoost分类器样本分布更新公式.gif)

从偏差-方差分解的角度看，Boosting算法主要关注降低偏差，因此能基于泛化性能较弱的学习器构建强的集成。

#### Bagging与随机森林
为了使基学习器有更大的差异，可以对训练集进行采样获得子集，用子集进行训练。而为了防止因为训练数据采样过度导致效果差，可以使用相互有交叠的采样子集。
###### Bagging
Bagging是并行式集成学习方法的代表。它基于自助采样法，初始训练集中约有63.2%的样本出现在采样集中。
基于T个上述的采样集分别训练基学习器，然后进行结合，就是Bagging的基本思想。
Bagging通常对分类任务使用简单投票法，对回归任务使用简单平均法。如果在分类中有两个类票数相同，最简单的是随机去一个，也可以考察投票的置信度。
Bagging算法的过程：
![西瓜书-8.Bagging算法.png](/img/西瓜书-8.Bagging算法.png)

Bagging是一个很高效的集成学习算法。
Bagging能够不经修改应用在多分类和回归任务上，而AdaBoost只适用于二分类任务。
自主采样给Bagging带来了另一个优点：由于每个学习器只是用了63.2%的样本，剩下的样本可用来做验证集（这就需要记住每个基学习器使用的训练样本），进行“外包估计”。
令D_t表示h_t实际使用的训练样本，H^oob(x)表示对样本x的外包预测（仅考虑未使用x的基学习器对x的预测），有
![西瓜书-8.Bagging外包预测.gif](/img/西瓜书-8.Bagging外包预测.gif)
Bagging泛化误差的外包估计为
![西瓜书-8.Bagging泛化误差的外包估计.gif](/img/西瓜书-8.Bagging泛化误差的外包估计.gif)

从偏差-方差分解的角度看，Bagging主要关注降低方差，因此在不剪枝决策树、神经网络等易受样本扰动的学习器上效果更明显。
###### 随机森林（RF）
随机森林是Bagging的一个扩展变体。它在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树训练过程中引入了随机属性选择。
传统决策树在选择划分属性时是在当前节点的属性集合中选择一个最优属性，而RF是对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，再从中选择一个最优属性。在这里，k控制了随机性的引入程度。一般推荐k = log_2 d。

随机森林简单、易实现、计算开销小，在很多现实任务中展现了强大的性能，被称为“代表集成学习技术水平的方法”。随机森林对Bagging做了小改动，其基学习器的多样性不仅来自数据扰动，还来自属性扰动。这使得最终集成的泛化性能可以进一步提升。
随机森林的收敛性与Bagging相似。其起始性能往往低于Bagging（特别是在只有一个基学习器时），但随着个体学习器数目增加，通常能收敛到更低的泛化误差。另外，随机森林的训练效率通常优于Bagging。
![西瓜书-8.RF与Bagging的比较.png](/img/西瓜书-8.RF与Bagging的比较.png)

#### 结合策略
学习器结合能够从三方面带来好处：

1. 从统计方面看，优于学习任务的假设空间往往很大，可能有多个假设在训练集上达到同样的性能，这时结合多学习器就能减小使用单学习器可能造成的误选导致泛化能力不佳的风险。
2. 从计算方面看，学习算法容易陷入局部极小解，对多个学习器的结合能够降低陷入糟糕的局部极小解的风险。
3. 从表示方面看，某些学习任务的真实假设可能不在当前的学习算法考虑的假设空间中，这时使用单学习器是无效的，而多学习器的结合能够学习到更好的近似。
![西瓜书-8.集成学习的三个方面的好处.png](/img/西瓜书-8.集成学习的三个方面的好处.png)

常用的结合策略有：

1. 平均法
    1. 简单平均法：直接计算平均
    2. 加权平均法：加权重
2. 投票法
    1. 绝对多数投票法：必须超过设定的票数（提供“拒绝预测”的选项）
    2. 相对多数投票法：取得票最多的。相同时随机取
    3. 加权投票法：加入权重
3. 学习法
训练数据很多时，学习法是更为强大的结合策略：通过另一个学习器来进行结合。代表算法：Stacking算法。
将个体学习器称为初级学习器，用于结合的学习器称为次级学习器或元学习器。Stacking算法先从初始数据集训练初级学习器，然后用初级学习器的输出作为输入特征、初始样本的标记为样例标记来生成新的数据集来训练次级学习器。
![西瓜书-8.集成学习的三个方面的好处.png](/img/西瓜书-8.集成学习的三个方面的好处.png)

次级学习器通常用交叉验证或留一法的方式，用训练初级学习器未使用的样本来产生训练样本。

#### 多样性

###### 误差-分歧分解
要构建泛化能力强的集成，个体学习器应该“好而不同”。
E = Ē - Ā
个体学习器准确性越高、多样性越大，则集成越好。

###### 多样性度量
多样性度量用于度量集成中个体分类器的多样性。
对于二分类任务，分类器h_i, h_j有以下预测结果联表：
![西瓜书-8.预测结果联表.png](/img/西瓜书-8.预测结果联表.png)
下面是一些常见的多样性度量：

* 不合度量：
![西瓜书-8.不合度量.gif](/img/西瓜书-8.不合度量.gif)
* 相关系数：
![西瓜书-8.相关系数.gif](/img/西瓜书-8.相关系数.gif)
* Q-统计量：
![西瓜书-8.Q-统计量.gif](/img/西瓜书-8.Q-统计量.gif)
* κ-统计量：
![西瓜书-8.κ-统计量.gif](/img/西瓜书-8.κ-统计量.gif)

![西瓜书-8.κ-误差图.png](/img/西瓜书-8.κ-误差图.png)

###### 多样性增强

* 数据样本扰动
* 输入属性扰动：数据集
* 输出表示扰动：对输出表示进行操纵，以增强多样性。
* 算法参数扰动：改变学习器中的一些参数