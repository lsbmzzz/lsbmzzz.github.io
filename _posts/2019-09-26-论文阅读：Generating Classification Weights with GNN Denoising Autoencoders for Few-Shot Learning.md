---
layout:     post
title:      论文阅读：Generating Classification Weights with GNN Denoising Autoencoders for Few-Shot Learning
subtitle:   论文阅读：Generating Classification Weights with GNN Denoising Autoencoders for Few-Shot Learning
date:       2019-09-26
author:     正版慕言
header-img: img/blog_bg_1.jpg
catalog: true
mathjax: true
tags:
    - Few-shot

---

对于给定的已经在基类上训练过的模型，这篇文章的工作目标是开发一个用于few-shot学习的元模型。元模型中有新类的输入，并且每一类的数据很少，因此必须以现有的模型为基础，调整为一个新的模型，使其能够正确对新类和基类进行统一的分类。

## 本文的两个主要创新：

1. 提出使用去燥自编码网络DAE，在训练期间以一组高斯噪声破坏的分类权值作为输入，学习和重建目标。
2. 为了在元模型给定的任务实例中获得不同类别之间的互相依赖关系，作者将DAE模型实现为一个图神经网络GNN。

## 主要目的：

学习出一个元模型，对于给出的一些只有少量训练样本的新类，在基类的分类权重基础上学习出新的权重向量，使之可以同时满足新类和基类的分类要求。

## 基于DAE的元模型：

对于分类问题，在训练过程中，我们往往需要较大量的数据才能学习到比较可靠的权重。为了解决这个问题，作者建立了一种基于DAE的元模型，在训练过程中DAE网络以一组被高斯噪声破坏过的分类权值作为输入，通过训练重建目标判别分类权重。

1. 注入噪声有助于学习过程的正则化，从而避免了过拟合风险。

2. 基于DAE的元模型能够通过计算输入权值和重构权值之间的差异，在给定的训练数据上近似出分类权值条件分布的梯度。

因此，从分类权重的初始估计开始，元模型能够执行梯度上升，根据训练数据将分类权重移动到更可能的位置。

为了在few-shot的条件下更好的应用DAE，作者对其进行了调整，通过使其重构更多的判别分类权重进一步提高了参数生成元任务的性能。

## 将DAE参数模型建立成图神经网络

考虑到一些类在语义上或视觉上比较相似，例如$Figure\ 1$中的鸟类、海洋生物。为了捕获这些不同类别之间的相互依赖关系，作者将DAE模型实现成了一个图神经网络GNN。GNN能够处理与图相关的一组无序的实体，这样再预测时模型就能够考虑到他们之间的关系。

![Figure 1](/img/Journal/NeuralNetworks/19.09.04-不同类别之间的相似性.png)

作者认为，所有类之间都存在依赖关系，包括新类和基类，并尝试通过GNN结构来获得这种依赖关系。GNN结构比简单的注意力机制更有表现力。作者为GNN提供了一些想要学习的类的分类权重的初始估计，通过训练得到更优的分类权重。最后，将GNN应用到一个不同但是相关的问题上而不考虑任何附带信息，因此这种方法对可以解决的问题领域更加不可预知。


## 核心思想

由于很多类之间具有相似性，它们的分类参数实际上也会有关联。因此，作者认为可以从已有的基类分类参数上学习新类的分类参数：

$$
w = \{w_i\}_{i=1}^{N=N_{bs}+N_{nv}} = g(D_{tr}^{nv}, w^{bs} | \phi)
$$

DAE是一种神经网络结构，当输入数据中有噪声时，它通过训练重构出干净的自编码器。研究表明当输入中加入了高斯噪声时，DAE模型能够通过输入w估计出能量函数梯度：

$$
\frac{\partial log \ p(w)}{\partial w} \approx\frac{1}{\sigma^2} \cdot (r(w) - w)
$$

$\sigma^2$是训练期间的噪声注入，$r()$为自动编码器。降噪自编码器的目标是复现出噪声污染之前的信号，但复现中几乎一定会存在偏差。理论表明，偏差向量可以近似的认为和特征密度分布函数的梯度项量一致。也就是说，上面的公式就是让w向密度分布高的方向走。

用DAE进行分类权重的生成：从一组初始化分类参数根据下面的梯度上升公式来调整$w$。其中$r(w)$是一个降噪编码的输出。初始化的参数这样定义：基类的参数就用基类的，新类的参数使用他们少量样本输出特征的平均值。

$$
w \leftarrow w + \epsilon \cdot \frac{\partial log \ p(w | D_{tr})}{\partial w} = w + \epsilon \cdot (r(w) - w)
$$

总结：根据已有的基类分类参数的数据分布，利用一个降噪自编码器去同时复现基类分类参数的分布和新类分类参数的分布。由于要复现出大量的基类分类参数分布信息，因此可以认为复现出的新数据的分布能够合理的反映出真实数据分布。

模型的整体结构：

![模型的整体结构](/img/NeuralNetworks/Journal/19.09.04-模型的整体结构.png)

元模型的结构如下：

![元模型的结构](/img/NeuralNetworks/Journal/19.09.04-元模型的结构.png)

GNN模型的结构如下：

![GNN模型的结构](/img/NeuralNetworks/Journal/19.09.04-GNN模型的结构.png)

训练的损失函数：

$$
\frac{1}{N}\sum_{i = 1}^{\tilde N} \| \hat {w_i} - w^\ast_i\|^2 + \frac{1}{m}\sum_{m=1}^M(x_m,y_m|\hat W)
$$

其中前半部分表示DAE的复现损失，后半部分表示M个样本使用参数$\hat w$的分类损失。