---
layout:     post
title:      7.机器学习|贝叶斯分类器
subtitle:   机器学习周志华 学习笔记————机器学习|贝叶斯分类器
date:       2019-07-30
author:     正版慕言
header-img: img/blog_bg_3.jpg
catalog: true
tags:
    - 机器学习
    - 西瓜书


---

#### 贝叶斯决策论
**贝叶斯决策论** 是概率框架下实施决策的基本方法。
贝叶斯判定准则：为最小化总体风险，只需在每个样本上选择那个能使条件风险最小的类别标记。
使用贝叶斯判定准则来最小化决策风险，首先要获得后验概率P(c|**x**)，然而在限时任务中通常难以直接获得。机器学习所要实现的就是基于有限的训练样本集尽可能准确地估计出后验概率P(c|**x**)。
有两种策略求后验概率：

1. 给定**x**，可以通过直接建模P(c|**x**)来预测c，叫做“判别式模型”。如决策树、BP神经网络、支持向量机等。
2. 先对联合概率分布P(**x**,c)建模，然后由此获得P(c|**x**)，叫做“生成式模型”。
![西瓜书-7.后验概率.gif](/img/MachineLearning/西瓜书-7.后验概率.gif)
用于归一化的“证据”因子P(**x**)，它与样本的类标记无关。
类先验概率P(c) 表达样本空间中各类样本所占的比例。
类条件概率P(**x**|c)，也叫**似然** 。

#### 极大似然估计
估计类条件概率的一种常用策略：先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。
概率模型的训练过程就是参数估计过程。
对于参数估计，统计学界分了两大学派：

* 频率主义学派认为参数是客观存在的固定值，可以通过优化似然函数等准则来确定参数值。
* 贝叶斯学派认为参数是未观察到的随机变量，其本身也可有分布，因此可以假定参数服从一个先验分布，然后基于观测到的数据来计算参数的后验分布。

**极大似然估计**（Maximum Likelihood Estimation, MLE） 源自频率主义学派，是根据数据采样来估计概率分布参数的经典方法。
令D_c表示训练集D种第c类样本组成的集合，假设它们是独立同分布的，参数θ_c对于数据集D_c的似然是：
![西瓜书-7.D_c的似然.gif](/img/MachineLearning/西瓜书-7.D_c的似然.gif)
对θ_c进行似然估计就是去寻找能最大化似然P的参数值。极大似然估计是要在θ_c的所有可能的取值中找到一个能使数据出现的可能性的最大值。
为了解决连乘操作下溢的问题，使用对数似然：
![西瓜书-7.D_c的对数似然.gif](/img/MachineLearning/西瓜书-7.D_c的对数似然.gif)

极大似然估计结果的准确率严重依赖所假设的概率分布形式是否符合潜在的真实数据分布。

#### 朴素贝叶斯分类器
基于贝叶斯公式估计后验概率的主要困难在于类条件概率P(**x**|c)是所有属性上的联合概率，难以从有限的训练样本直接估计得到。
朴素贝叶斯分类器（naive Bayes classifier）采用了“属性条件独立性假设”的方法：对已知类别假设所有属性相互独立。基于这一假设，贝叶斯公式可以写为
![西瓜书-7.属性独立的贝叶斯公式.gif](/img/MachineLearning/西瓜书-7.属性独立的贝叶斯公式.gif)
朴素的贝叶斯分类器表达式：
![西瓜书-7.朴素的贝叶斯分类器.gif](/img/MachineLearning/西瓜书-7.朴素的贝叶斯分类器.gif)
其训练过程是基于训练集D来估计先验概率P(c)并为每个属性估计条件概率P(x_i | c)。
对D中的第c类样本，类先验概率为：
![西瓜书-7.D的类先验概率.gif](/img/MachineLearning/西瓜书-7.D的类先验概率.gif)
对于离散属性，有
![西瓜书-7.D离散的类先验概率.gif](/img/MachineLearning/西瓜书-7.D离散的类先验概率.gif)
对于连续属性可以考虑概率密度函数，有
![西瓜书-7.D连续的类先验概率.gif](/img/MachineLearning/西瓜书-7.D连续的类先验概率.gif)
如果某个属性值在训练集中没有与某一类同时出现过，基于条件概率公式估计再根据贝叶斯分类器进行分类就会出现概率估值为0问题。为了避免其他属性携带的信息被训练集中未出现的属性值抹掉，在估计概率值时通常用**拉普拉斯修正**进行进行“平滑”：拉普拉斯修正将先验概率修正为：
![西瓜书-7.拉普拉斯修正先验概率.gif](/img/MachineLearning/西瓜书-7.拉普拉斯修正先验概率.gif)

#### 半朴素贝叶斯分类器
对朴素贝叶斯分类器中属性条件独立性假设进行一定程度的放松，得到般朴素贝叶斯分类器。
基本思想：适当考虑一部分属性间的相互依赖信息，从而既不需要进行完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系。最常用的策略是“独依赖估计”。
“独依赖”假设每个属性在类别之外最多只依赖一个其他属性。
![西瓜书-7.独依赖估计.gif](/img/MachineLearning/西瓜书-7.独依赖估计.gif)
式中pa_i是属性x_i依赖的属性，称为x_i的父属性。如果父属性已知，就能够根据拉普拉斯修正的先验概率公式估计概率值P(x_i|c,pa_i)。
确定每个属性父属性的方法：
![西瓜书-7.属性依赖关系.png](/img/MachineLearning/西瓜书-7.属性依赖关系.png)

1. 最直接的方法是假设所有属性都依赖同一个属性，称为“超父”，然后通过交叉验证来确定超父。称为SPODE（Super-Parent ODE）方法。
2. TAN（Tree Augmented naive Bayes）方法，在最大带权生成树的基础上通过如下操作生成树形结构：
    1. 计算任意两个属性间的条件互信息
        ![西瓜书-7.条件互信息.gif](/img/MachineLearning/西瓜书-7.条件互信息.gif)
    2. 以属性为节点构建完全图，任意两点间的权重设为I(x_i, x_j \| y)
    3. 构建次完全图的最大带权生成树，挑选根变量，将边设置为有向
    4. 加入类别节点y，增加从y到每个属性的有向边
    
    条件互信息I(x_i, x_j \| y)描述属性x_i和x_j在已知类别情况下的相关性。通过最大生成树算法，TAN实际上只保留了相关属性之间的依赖性。
3. AODE（Averaged One-Dependent Estimator），是一种基于集成学习机制、更强大的独依赖分类器。尝试将每个属性作为超父来构建SPODE，然后将具有足够训练数据支撑的SPODE集成起来作为最终结果。
    ![西瓜书-7.AODE.gif](/img/MachineLearning/西瓜书-7.AODE.gif)
    在公式1中，m'是阈值常数。
    AODE显然需要估计P(c, x_i)和P(x_j | c, x_i)。N是D中可能的类别数，N_i是第i个属性肯能的取值数，D_c,xi 是类别为c且在第i个属性上取值为xi的样本集合，D_c,xi,xj 是类别为c且在第i和第j个属性上取值为xi，xj的样本的集合。
    AODE和朴素贝叶斯分类器类似，其训练过程也是计数的过程。
    
#### 贝叶斯网
贝叶斯网又叫“信念网”，是借助有向无环图（DAG）来刻画属性之间的依赖关系并使用条件概率表来描述属性的联合概率分布。
一个贝叶斯网B由结构G和参数Θ两部分构成（B = 〈G, Θ〉）。G是有向无环图，Θ定量描述这种依赖关系。
![西瓜书-7.贝叶斯网条件概率表.gif](/img/MachineLearning/西瓜书-7.贝叶斯网条件概率表.gif)

1. 结构
贝叶斯网结构有效地表达了属性之间的条件独立性，给定父节点集，贝叶斯网假设每个属性与它的非后裔属性独立。有
![西瓜书-7.贝叶斯网联合概率分布.gif](/img/MachineLearning/西瓜书-7.贝叶斯网联合概率分布.gif)
贝叶斯网中三个变量之间的典型依赖关系
![西瓜书-7.贝叶斯网中三个变量的典型依赖关系.png](/img/MachineLearning/西瓜书-7.贝叶斯网中三个变量的典型依赖关系.png)
    
2. 学习
在现实任务中，我们往往不知道贝叶斯网的结构，因此，贝叶斯网学习的首要任务是根据训练数据集找出结构最“恰当”的贝叶斯网。
常用的办法：“评分搜索”。定义一个评分函数，以此来评估贝叶斯网与训练数据的契合程度，然后基于这个评分函数来寻找结构最优的贝叶斯网。
通常评分函数的学习目标是找到一个能够以最短的编码长度描述训练数据的模型。我们应该选择综合编码长度（包括网络描述和编码数据）最短的贝叶斯网（“最小描述长度，MDL”）。
给定一个训练集D，贝叶斯网B在D上的评分函数：
![西瓜书-7.贝叶斯网评分函数.gif](/img/MachineLearning/西瓜书-7.贝叶斯网评分函数.gif)
其中|B|是贝叶斯网的参数个数，f(θ)表示描述每个参数θ的字节数，LL是贝叶斯网的对数似然。
![西瓜书-7.贝叶斯网对数似然.gif](/img/MachineLearning/西瓜书-7.贝叶斯网对数似然.gif)
存在的问题：从所有可能的网络结构空间搜索最优贝叶斯网结构是一个NP难问题，难以快速求解。对此有两种常用的策略：
    * 贪心法
    * 给网络结构施加约束

3. 推断
通过已知变量观测值来推测待查询变量的过程称为“推断”，已知变量观测值称为“证据”。
最理想的情况是直接根据贝叶斯网定义的联合概率分布来精确计算后验概率，但这是NP难问题，我们需要“近似推断”。现实中常使用 **吉布斯采样** 来完成。
过程：![西瓜书-7.吉布斯采样算法.png](/img/MachineLearning/西瓜书-7.吉布斯采样算法.png)
先随机产生一个与证据**E = e**一致的样本**q**^0作为初始点，每步从当前样本出发产生下一个样本。在第t次采样中，先假设**q**^t = **q**^(t - 1)，然后对非证据变量逐个进行采样改变其取值，采样概率根据贝叶斯网B和其它变量的当前取值（**Z = z**）计算获得。假设经过T次采样得到的与**q**一致的样本有n_q个，可以近似估算出后验概率
![西瓜书-7.吉布斯采样后验概率.gif](/img/MachineLearning/西瓜书-7.吉布斯采样后验概率.gif)
吉布斯采样的收敛速度较慢。另外，如果贝叶斯网中存在0或1的极端概率分布，则不能保证马尔科夫链存在平稳分布，此时吉布斯采样会给出错误的估计结果。

#### EM算法
现实应用中往往会遇到训练样本中属性变量的值不完整的情况。这种变量叫做“隐变量”。
让**X**表示观测到的变量集，**Z**表示隐变量集，用Θ表示模型参数。Θ的对数似然为
![西瓜书-7.Θ的对数似然.gif](/img/MachineLearning/西瓜书-7.Θ的对数似然.gif)
由于**Z**是隐变量，我们可以通过对**Z**计算期望来最大化已观测数据的对数“边际似然”：
![西瓜书-7.Θ的边际似然.gif](/img/MachineLearning/西瓜书-7.Θ的边际似然.gif)
EM算法是常用的估计参数隐变量的工具。其基本思想：如果参数Θ已知，则根据训练数据推断出最优隐变量**Z**的值；若**Z**已知，则可以对参数Θ做最大似然估计。
对于初始值Θ^0，可以迭代执行如下步骤到收敛：

* 基于Θ^t推断隐变量**Z**的值记为**Z**^t
* 基于已观测变量**X**和**Z**^t对参数Θ做极大似然估计记为Θ^(t + 1)

EM算法的两个步骤：

* E：以当前参数Θ^t推断隐变量分布P(**Z**,  \|**X**, Θ^t)，并计算对数似然LL(Θ\|**X**, **Z**)关于**Z**的期望
    * ![西瓜书-7.E步.gif](/img/MachineLearning/西瓜书-7.E步.gif)
    
* M：寻找参数最大化期望似然
    * ![西瓜书-7.M步.gif](/img/MachineLearning/西瓜书-7.M步.gif)
    
EM算法是一种非梯度优化方法。