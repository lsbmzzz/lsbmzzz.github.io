---
layout:     post
title:      12.机器学习|计算学习理论
subtitle:   机器学习周志华 学习笔记————机器学习|计算学习理论
date:       2019-08-01
author:     正版慕言
header-img: img/blog_bg_3.jpg
catalog: true
tags:
    - 机器学习
    - 西瓜书

---

#### 基础知识
计算学习理论研究的是关于通过“计算”来进行“学习”的理论，即关于机器学习的理论基础。其目的是分析学习任务的困难本质，为学习算法提供理论保证，并根据分析结果指导算法设计。

主要讨论二分类问题，对于样例集D，应h为从X到Y的一个映射，其泛化误差和在D上的经验误差分别为：
![西瓜书-12.泛化误差与经验误差](/img/MachineLearning/西瓜书-12.泛化误差与经验误差.gif)

后面将研究经验误差与泛化误差之间的逼近程度。如果h在数据集D上的经验误差为0，称h与D一致，否则说h与D不一致。对任意两个映射h1,h2 ∈X→Y，可通过其“不合”度量他们之间的差别：
![西瓜书-12.不合](/img/MachineLearning/西瓜书-12.不合.gif)

几个常用的不等式：

* Jensen不等式：对任意凸函数f(x)，有
![西瓜书-12.Jensen不等式](/img/MachineLearning/西瓜书-12.Jensen不等式.gif)
* Hoeffding不等式：若x1, x2, ..., xm为m个独立随机变量，且满足0≤xi≤1，对任意ε有
![西瓜书-12.Hoeffding不等式](/img/MachineLearning/西瓜书-12.Hoeffding不等式.gif)
* McDiarmid不等式：若x1, x2, ..., xm为m个独立随机变量，且对任意0≤i≤m满足
![西瓜书-12.McDiarmid不等式f条件.gif](/img/MachineLearning/西瓜书-12.McDiarmid不等式f条件.gif)
则对任意ε>0有
![西瓜书-12.McDiarmid不等式.gif](/img/MachineLearning/西瓜书-12.McDiarmid不等式.gif)

#### PAC学习
计算学习理论中最基本的是概率近似正确(PAC)学习理论。
令c表示概念，若对训练集D有任意c(**x**) = y成立，称c为目标概念。我们希望学得的目标概念构成的集合称为“概念类”，用C表示。
给定学习算法￡它考虑的所有可能概念的集合称为“假设空间”，用H表示。对于其中的任一概念，用h表示。由于不能确定它是否真的是目标概念，因此称为“假设”。
如果目标概念c∈H，则H中存在假设能将所有示例按与真实标记一致的方式完全分开，我们就称该问题对学习算法￡是“可分的”。反之，称为“不可分的”。
给定训练集D，我们希望基于学习算法￡学到的模型对应的假设h尽可能接近目标概念c。以较大的概率学得误差满足预设上限的模型就是“概率”“近似正确”的含义。

> 定义 **PAC辨识**：对0 < ε， δ < 1，所有c∈C和分布D，若存在学习算法￡，其输出假设h∈H满足
> P(E(h) ≤ ε) ≥ 1 - δ，
> 则称学习算法￡能从假设空间H中PAC辨识概念类C。

在此基础上可以定义：

> 定义 **PAC可学习**：令m表示从分布D中独立同分布采样得到的样例数目，0 < ε， δ < 1，对所有分布D，若存在学习算法￡和多项式函数poly(·,·,·,·)，使对于任何m ≥ploy(1/ε, 1/δ, size(**x**), size(c)), ￡能从假设空间H中PAC辨识概念类C，则称概念类C对假设空间H而言是PAC可学习的，有时也简称概念类C是PAC可学习的。

对计算机来说，要考虑算法的时间复杂度，因此：
> 定义 **PAC学习算法**： 若学习算法￡使概念类C为PAC可学习的，且￡的运行时间也是多项式函数ploy(1/ε, 1/δ, size(**x**), size(c)),则称概念类C是高效PAC可学习的，称￡为概念类C的PAC学习算法。

假定学习算法￡处理每个样本的时间为常数，则￡的时间复杂度等价于样本复杂度。于是，我们关心的时间复杂度就变成了样本复杂度。

> 定义 **样本复杂度**：满足PAC学习算法￡所需的m≥ploy(1/ε, 1/δ, size(**x**), size(c))中最小的m，称为学习算法￡的样本复杂度。

PAC学习中的一个关键因素是假设空间H的复杂度，其中包含了学习算法￡所有可能输出的假设。如果PAC学习中假设空间与概念类完全相同，称为“恰PAC可学习”。但通常情况下我们要研究H≠C的情况。|H|有限时称H为“有限假设空间”，无限时称为“无限假设空间”。

#### 有限假设空间

1. 可分情形
可分情形意味着c∈H。
给定包含m个样本的训练集D，一个简单的学习策略是：既然D中的样例标记都是目标概念c赋予的，并且c存在假设空间H中，那么在D上标记错误的假设肯定不是c。所以，只要保留与D一致的假设即可。当D足够大，最终只会剩下目标概念c。但通常D的大小都是有限的，我们只需要找到目标概念的有效近似。所以，当学习算法￡能达到PAC可学习即可。

有限假设空间H都是PAC可学习的，所需样例数目如下：
![西瓜书-12.PAC所需样例数.gif](/img/MachineLearning/西瓜书-12.PAC所需样例数.gif)
输出假设h的泛化误差随样例数增加收敛到0，首先速度O(1/m)。

2. 不可分情形
对于较为困难的学习问题，目标概念c往往不存在假设空间H中。假设空间H的任意一个假设都会在训练集上出现或多或少的错误，由Hoeffding不等式可知：
> **引理** 若训练集D包含m个从分布D删独立同分布采样而得的样例，0 < ε < 1，则对任意h∈H有
> ![西瓜书-12.不可分-引理.gif](/img/MachineLearning/西瓜书-12.不可分-引理.gif)

>**推论** 若训练集D包含m个从分布D上独立同分布采样而得的样例，0 < ε < 1，则对任意h∈H，下式以至少1-δ的概率成立：
>![西瓜书-12.不可分-推论.gif](/img/MachineLearning/西瓜书-12.不可分-推论.gif)

这表明当样例数目m较大时，h的经验误差是其泛化误差很好的近似。对于有限假设空间H，有

> **定理** 若H为有限假设空间，0 < δ < 1，则对任意h∈H，有
> ![西瓜书-12.不可分-定理.gif](/img/MachineLearning/西瓜书-12.不可分-定理.gif)

显然，当c∉H时，学习算法￡无法学得目标概念c的ε近似。但当H给定时，其中必然存在一个泛化误差最小的假设，找出这个假设的ε近似也是一个较好的目标。以此为目标，可以将PAC学习推广到ε∉H的“不可知学习”：
> 定义 **不可知PAC学习**： 令m表示从分布D中独立同分布采样得到的样例数目，0 < ε， δ < 1，对所有分布D，若存在学习算法￡和多项式函数poly(·,·,·,·)，使对于任何m ≥ploy(1/ε, 1/δ, size(**x**), size(c)), ￡能从假设空间H中输出满足
> ![西瓜书-12.不可知PAC可学习.gif](/img/MachineLearning/西瓜书-12.不可知PAC可学习.gif)
> 的假设h，则称假设空间H是不可知PAC可学习的。

#### VC维
现实任务中面对的通常是无线假设空间，因此需要度量假设空间的复杂度。最常见的是考虑假设空间的“VC维”。
在VC维之前，先引入几个概念：**增长函数、对分、打散**。

> **定义** 对所有m∈**N**，假设空间H的增长函数为
> ![西瓜书-12.增长函数.gif](/img/MachineLearning/西瓜书-12.增长函数.gif)

增长函数描述了H的表示能力，由此反映出假设空间的复杂度。我们可以利用增长函数来估计经验误差与泛化误差之间的关系：

> **定理** 对假设空间H，m∈**N**，0 < ε < 1和任意h∈H，有
> ![西瓜书-12.增长函数估计经验误差与泛化误差关系.gif](/img/MachineLearning/西瓜书-12.增长函数估计经验误差与泛化误差关系.gif)

对二分类问题，H中的假设对D中示例赋予标记的每种可能结果称为对D的一种“**对分**”。

若假设空间H能实现示例集D上的所有对分，即对m个示例的D其增长函数等于2^m，称D能被H“**打散**”。

> **定义** 假设空间H的VC维时能被H打散的最大示例集的大小：
> ![西瓜书-12.最大示例集.gif](/img/MachineLearning/西瓜书-12.最大示例集.gif)

VC维与增长函数有密切联系，下面给出二者之间的定量关系：

> **引理** 若假设空间H的VC维为d，则对任意m∈**N**有
> ![西瓜书-12.VC维与增长函数的定量关系.gif](/img/MachineLearning/西瓜书-12.VC维与增长函数的定量关系.gif)

由此可以计算出增长函数的上界

> **推论** 若假设空间H的VC维为d，则对任意m ≥d有
> ![西瓜书-12.增长函数上界.gif](/img/MachineLearning/西瓜书-12.增长函数上界.gif)

进而得到基于VC维的泛化误差界

> **定理** 若假设空间H的VC维为d，则对任意m > d, 0 < δ < 1 和h∈H，有
> ![西瓜书-12.VC维的泛化误差界.gif](/img/MachineLearning/西瓜书-12.VC维的泛化误差界.gif)

可以看出，泛化误差界只与m有关，收敛速度为O(1/√m)。
令h表示学习算法￡输出的假设，如果h满足
![西瓜书-12.使￡经验风险最小化的h.gif](/img/MachineLearning/西瓜书-12.使￡经验风险最小化的h.gif)
则称￡满足经验风险最小化（ERM）原则的算法。有
> **定理** 任何VC维有限的假设空间H都是（不可知）PAC可学习的。

#### Rademacher复杂度

基于VC维的泛化误差界是分布无关、数据独立的，对任何数据分布都成立。这使基于VC维的可学习性分析结果具有一定的普适性。但另一方面，由于没有考虑数据自身，得到的泛化误差界通常比较“松”。
Rademacher复杂度是另一种刻画假设空间复杂度的途径，在一定程度上考虑了数据分布。

考虑实值函数空间F : Z→**R**，有
> **定义** 函数空间F关于Z的经验Rademacher复杂度
> ![西瓜书-12.经验Rademacher复杂度.gif](/img/MachineLearning/西瓜书-12.经验Rademacher复杂度.gif)

经验Rademacher复杂度衡量了F与随机噪声在集合Z中的相关性。我们希望了解函数空间F在Z上关于分布D的相关性，因此对所有从D独立同分布采样得到的大小为m的集合Z求期望，有
> **定义** 函数空间F关于Z上的分布D的Rademacher复杂度
> ![西瓜书-12.F的Rademacher复杂度.gif](/img/MachineLearning/西瓜书-12.F的Rademacher复杂度.gif)

基于Radermacher复杂度可得关于F的泛化误差界

> **定理** 对实值函数空间F：Z→[0, 1]，根据分布D从Z中独立同分布采样得到示例集Z={z1, z2, ..., zm}，z1∈Z，0 < δ < 1，对任意f∈F，以至少1 - δ的概率，有
> ![西瓜书-12.F的泛化误差界.gif](/img/MachineLearning/西瓜书-12.F的泛化误差界.gif)

上述定理由于F是[0,1]上的实值函数，因此只适用于回归问题。对于二分类问题，有：

> **定理** 对假设空间H：X→{-1，+1}，根据分布D从X中独立同分布采样得到示例集D = {x1, x2, ..., xm}，xi∈X， 0 < δ < 1，对任意h∈H，以至少1 - δ的概率，有
> ![西瓜书-12.F二分类的泛化误差界.gif](/img/MachineLearning/西瓜书-12.F二分类的泛化误差界.gif)

关于Rademacher复杂度与增长函数，有如下定理：

> **定理** 假设空间H的Rademacher复杂度Rm（H）与增长函数ΠH（m）满足
> ![西瓜书-12.RmH与ΠHm的关系.gif](/img/MachineLearning/西瓜书-12.RmH与ΠHm的关系.gif)

#### 稳定性
算法的“稳定性”考察的是算法在输入发生变化时，输出是否会随之发生较大的变化。

定义两种D的变化方式和三种损失：
D^\i表示移除第i个样例，D^i表示替换第i个样例
泛化损失、经验损失、留一损失
![西瓜书-12.稳定性-定义.gif](/img/MachineLearning/西瓜书-12.稳定性-定义.gif)

均匀稳定性：
> **定义** 对任何**x**∈X，**z**=（**x**，y），若学习算法￡满足
> ![西瓜书-12.β均匀稳定性.gif](/img/MachineLearning/西瓜书-12.β均匀稳定性.gif)
> 称￡关于损失函数满足β-均匀稳定性。

若损失函数游街，则有

> **定理** 给定从分布D上独立同分布采样得到的大小为m的示例集D，若学习算法￡满足关于损失函数的β-均匀稳定性，且损失函数的上界为M，0 < δ < 1，则对任意m≥1，以至少1-δ的概率，有
> ![西瓜书-12.￡泛化误差界.gif](/img/MachineLearning/西瓜书-12.￡泛化误差界.gif)

对损失函数若学习算法￡输出的假设满足经验损失最小化，称算法￡满足经验风险最小化原则（ERM）。关于算法的稳定性和可学习性，有

> **定理** 若学习算法￡是ERM且稳定的，则假设空间H可学习。