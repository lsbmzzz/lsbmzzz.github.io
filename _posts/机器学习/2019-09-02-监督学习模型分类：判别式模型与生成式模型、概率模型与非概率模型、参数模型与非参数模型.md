---
layout:     post
title:      监督学习模型分类：判别式模型与生成式模型、概率模型与非概率模型、参数模型与非参数模型
subtitle:   监督学习模型分类：判别式模型与生成式模型、概率模型与非概率模型、参数模型与非参数模型
date:       2019-09-02
author:     正版慕言
header-img: img/blog_bg_1.jpg
catalog: true
mathjax: true
tags:
    - 机器学习

---

问题设定：假设二分类问题的输入空间（特征空间）为欧式空间$X$，维度是$d$，$x$为输入空间中的一个随机变量，输出空间为$Y(y = {-1, 1})$，$P(x,y)$是$x,y$的联合分布，而我们不知道这个联合分布的形式。这个分布产生了$m$个样例构成训练集$D$。

# 判别式模型和生成式模型

#### 判别式模型

可以分为两种：

1. 直接对输入空间到输出空间的映射关系进行建模：

$$h:X \rightarrow Y, \ y = h(x) $$

2. 对条件概率$P(y \| x) $进行建模，然后根据贝叶斯风险最小化准则进行分类：

$$y = arg \max_{y \in \{-1, 1\}} P(y | x) $$

判别式模型有感知机、支持向量机、神经网络、逻辑回归、k近邻等

#### 生成式模型：先对$P(x, y) $进行建模，然后根据贝叶斯公式

$$P(y | x) = \frac{P(x | y) P(y)}{P(x)} $$

其中

$$P(x,y) = P(x | y)P(y) $$

算出$P(y\|x) $，然后根据

$$y = arg \max_{y \in \{-1, 1\}} P(y | x) $$

做分类。

生成式模型有高斯判别分析、朴素贝叶斯等

# 概率模型和非概率模型

#### 非概率模型

非概率模型指直接学习输入空间到输出空间的映射，学习过程基本不涉及概率密度的估计和积分等操作。问题的关键在于最优化问题的求解。

为了学习假设$h(x)$，我们先根据先验知识选择一个特定的假设空间H，在这个空间中找出泛化误差最小的假设

$$h^* = arg\min_{h \in H} \epsilon(h) = arg\min_{h \in H} \sum_{x,y}l(h(x), y)P(x,y) $$

其中$l(h(x), y) $是我们选择的损失函数。我们不知道$P(x,y) $，所以去找使得经验误差最小化的假设

$$g = arg\min_{h \in H}\hat\epsilon(h) = arg\min_{h \in H}\frac{1}{m}\sum_{i = 1}^ml(h(x)^{(i)},y^{(i)}) $$

这种学习策略叫做经验误差最小化(ERM)。我们需要有先验知识来选择假设空间。这些先验知识还可以用在正则化上。

正则化是对模型复杂度的惩罚项，防止模型的过拟合。在经验误差的基础上加上正则化项，将两者同时最小化，这种学习策略叫做结构风险最小化(SRM)。

[L1、L2正则化](https://lsbmzzz.github.io/2019/08/26/L1-L2%E6%AD%A3%E5%88%99%E5%8C%96/)

非概率模型有感知机、支持向量机、神经网络、k近邻等。

#### 概率模型

概率模型指出学习目的是学出$P(x,y), P(y\|x) $，但最后都根据后者来做判别。

逻辑回归、高斯判别分析、朴素贝叶斯都属于概率模型。

# 参数模型和非参数模型

#### 参数模型

参数模型需要我们对问题具有一定的先验知识，假设出要学习的目标函数或分布的具体形式，然后通过训练数据及估算出函数中的未知参数。

参数模型一个很重要的特点就是如果模型的假设正确，可以通过很小的训练集获得很好的模型；而如果假设错误，大量的训练数据也不能弥补假设模型和实际模型之间的偏差。

#### 非参数模型

如果我们对要学习的问题不够了解，一般不能做出模型假设。面对预测任务，我们通常用上所有训练数据。这样建立的模型就是非参数模型。

非参数模型的特点是存储和计算开销都很大，但由于不存在模型假设问题，可以证明当训练量足够大时模型可以逼近任意复杂的实际模型。

非参数模型中一般会含有一个或数个超参数和无数普通参数。

例如深度学习。