---
layout:     post
title:      9.机器学习|聚类
subtitle:   机器学习周志华 学习笔记————机器学习|聚类
date:       2019-07-30
author:     正版慕言
header-img: img/blog_bg_3.jpg
catalog: true
tags:
    - 机器学习
    - 西瓜书

---

#### 聚类
在无监督学习中，训练样本的标记信息是未知的，要通过学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础。聚类是这类学习任务中研究最多应用最广的方法。

聚类试图将数据集中的样本划分为若干通常不相交的子集，每个子集称为一个“簇”。

聚类可以作为一个单独的过程用于寻找数据内在的分布结构，也可以作为分类等其他学习任务的前趋过程。

聚类算法设计的两个基本问题：性能度量和距离计算

#### 性能度量
聚类的性能度量又叫“有效性指标”。对聚类结果，我们需要通过某种性能度量来评估其好坏；另一方面，如果明确了最终要使用的性能度量，就可以直接将其作为聚类过程的优化目标。
聚类结果的“簇内相似度”要高，“簇外相似度”要低。
聚类性能度量有两类：一类是将聚类结果与某个“参考模型”进行比较，称为“外部指标”；一类是直接考察聚类结果而不利用任何参考模型，称为“内部指标”。

常用的外部指标：

* Jaccard系数（JC）：越大越好
![西瓜书-9.JC.gif](/img/机器学习/西瓜书/西瓜书-9.JC.gif)

* FM指数（FMI）：越大越好
![西瓜书-9.FMI.gif](/img/机器学习/西瓜书/西瓜书-9.FMI.gif)

* Rand指数（RI）：越大越好
![西瓜书-9.RI.gif](/img/机器学习/西瓜书/西瓜书-9.RI.gif)

常用的内部指标：

* DB指数（DBI）：越小越好
![西瓜书-9.DBI.gif](/img/机器学习/西瓜书/西瓜书-9.DBI.gif)

* Dunn指数（DI）：越大越好
![西瓜书-9.DI.gif](/img/机器学习/西瓜书/西瓜书-9.DI.gif)

#### 距离计算
对函数dist(·,·)，如果它是一个“距离度量”，那么它需要满足：

* 非负性
* 统一性
* 对称性
* 直递性

最常用的是“闵可夫斯基距离”：
![西瓜书-9.闵可夫斯基距离.gif](/img/机器学习/西瓜书/西瓜书-9.闵可夫斯基距离.gif)
当p为2时是欧氏距离，p为1时是曼哈顿距离。
闵可夫斯基距离可用于有序属性，不能用于无序属性。

对无序属性可以采用VDM（Value Difference Metric）：
![西瓜书-9.VDM.gif](/img/机器学习/西瓜书/西瓜书-9.VDM.gif)

将闵可夫斯基距离和VDM距离结合，就能够处理混合属性。在n个属性中，假设前c个是有序属性，那么
![西瓜书-9.Minkowski+VDM.gif](/img/机器学习/西瓜书/西瓜书-9.Minkowski+VDM.gif)

属性的重要性不同时，可以使用加权距离。

#### 原型聚类
也叫作“基于原型的聚类”。通常，算法先对原型进行初始化，然后对原型进行迭代更新求解。采用不同的原型表示和不同的求解方式，会产生不同的算法。常用的几种有：k均值算法、学习向量量化、高斯混合聚类。

###### k均值算法
k均值算法针对聚类所得的簇划分，最小化平方误差
![西瓜书-9.k均值E.gif](/img/机器学习/西瓜书/西瓜书-9.k均值E.gif)
E越小，簇内样本相似度越高。

k均值算法过程：
![西瓜书-9.k均值算法.png](/img/机器学习/西瓜书/西瓜书-9.k均值算法.png)
![西瓜书-9.西瓜数据集与k均值的结果.png](/img/机器学习/西瓜书/西瓜书-9.西瓜数据集与k均值的结果.png)

###### 学习向量量化（Learning Vector Quantization, LVQ）
LVQ假设数据样本带有类别标记，学习过程利用这些监督信息来辅助聚类。算法过程如下：
![西瓜书-9.LVQ算法.png](/img/机器学习/西瓜书/西瓜书-9.LVQ算法.png)
对于给定的样本集D，LVQ的目标是学得一组原型向量{p1, p2, ..., pq}，每个原型向量代表一个聚类簇。（初始的原型向量随机选择）
LVQ的关键在于如何更新原型向量，算法的核心在第6-10行。

###### 高斯混合聚类
高斯混合聚类采用概率模型来表达聚类原型。
高斯分布概率密度函数：
![西瓜书-9.高斯分布概率密度函数.gif](/img/机器学习/西瓜书/西瓜书-9.高斯分布概率密度函数.gif)
定义高斯混合分布：
![西瓜书-9.高斯混合分布.gif](/img/机器学习/西瓜书/西瓜书-9.高斯混合分布.gif)
包括了k个混合成分，每个混合成分对应一个高斯分布。μi和**Σi**是第i个混合成分的参数，αi > 0是“混合系数”，αi累加 = 1。

高斯混合模型的EM算法：
在每一步迭代中，先根据当前参数计算每个样本属于每个高斯成分的后验概率γ_ji（E），再根据下式更新模型参数{(αi, μi, Σi) | 1 ≤ i ≤ k}：
![西瓜书-9.高斯混合分布-更新参数.gif](/img/机器学习/西瓜书/西瓜书-9.高斯混合分布-更新参数.gif)
算法流程：
![西瓜书-9.高斯混合聚类算法.png](/img/机器学习/西瓜书/西瓜书-9.高斯混合聚类算法.png)

#### 密度聚类
密度聚类又称为“基于密度的聚类”，假设聚类结构能通过样本分布的紧密程度决定。通常情况下密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇以获得最终的聚类效果。
DBSCAN是一种著名的密度聚类算。它基于一组“邻域”参数（ε，MinPts）来刻画样本分布的紧密程度。给定的数据集D，有如下几个概念：

* ε-邻域：对**x**j∈D，其ε-邻域包含样本集D中与**x**j的距离不大于ε的样本。
* 核心对象：若**x**j的ε-邻域至少包含MinPts个样本，则**x**j是一个核心对象。
* 密度直达：若**x**j位于**x**i的ε-邻域中，且**x**i是核心对象，则称**x**j由**x**i密度直达。
* 密度可达：对**x**i和**x**j，若存在样本序列**p**1, **p**2, ..., **p**n, 其中**p**1 = **x**i，**p**n = **x**j，且**p**i+1由**p**i密度直达，则称**x**j由**x**i密度可达。
* 密度相连：对**x**i与**x**j，若存在**x**k使**x**i与**x**j均由**x**k密度可达，则称**x**i与**x**j密度相连。
![西瓜书-9.密度聚类概念展示.png](/img/机器学习/西瓜书/西瓜书-9.密度聚类概念展示.png)

基于以上概念，DBSCAN将簇定义为：由密度可达关系导出的最大的密度相连样本集合。给定邻域参数（ε，MinPts），簇C⊆D是满足以下性质的非空样本子集：

* 连接性：**x**i∈C，**x**j∈C → **x**i与**x**j密度相连
* 最大性：**x**i∈C，**x**j由**x**i密度可达 → **x**j∈C

![西瓜书-9.DBSCAN算法.png](/img/机器学习/西瓜书/西瓜书-9.DBSCAN算法.png)

#### 层次聚类
层次聚类试图在不同层次对数据集进行划分，从而形成树形的聚类结构。划分可以采用自底向上的聚合策略和自顶向下的拆分策略。

AGNES是一种自底向上的聚合策略的层次聚类算法。它现将数据集中的每一个样本看做一个初始聚类簇，然后再算法运行的每一步找出距离最近的两个簇进行合并，不断重复达到预设的簇个数。这里的关键是如何计算距离。
实际上，每一个簇都是一个样本的集合，因此只需要采用关于集合的某种距离即可。例如对于给定的聚类簇Ci和Cj，可以这样计算：
![西瓜书-9.层次聚类-距离计算.gif](/img/机器学习/西瓜书/西瓜书-9.层次聚类-距离计算.gif)
![西瓜书-9.AGNES算法.png](/img/机器学习/西瓜书/西瓜书-9.AGNES算法.png)

层次聚类的树状图样例：
![西瓜书-9.AGNES算法的树状图样例.png](/img/机器学习/西瓜书/西瓜书-9.AGNES算法的树状图样例.png)

当聚类簇距离用min、max或avg计算时，AGNES算法相应的被称为“单链接”、“全连接”或“均连接”算法。
