---
layout:     post
title:      Softmax的理解
subtitle:   Softmax的理解
date:       2019-08-20
author:     正版慕言
header-img: img/blog_bg_1.jpg
catalog: true
tags:
    - 机器学习

---

> Softmax又称为归一化函数，在机器学习中有广泛的应用，它能够将一个实数向量“压缩”到另一个相同维度的向量中，使每个元素的范围都在0到1之间且所有元素的和为1。

## softmax的定义
假设有数组V，那么这个数组中第i个元素的值为
![6900ef44f5df25faca79f1f6fbb03438.gif](/img/机器学习/softmax.gif)
softmax的理解非常直观，其输出可以看做分类概率来理解，因此很适用于多分类任务。

一张知乎上烂大街的图解：
![79e5089ee0644a8af35ee86592de908a.jpeg](/img/机器学习/softmax图解.jpg)

## softmax求导
如果我们要通过梯度下降对分类loss进行改进，就需要求loss对每个权重的偏导，采用链式法则反向传播。
这时候第一步工作就是对softmax求导。
![55754500685ebc473d963d39b5d99dad.png](/img/机器学习/神经网络softmax.png)
对于上面一个简单的例子，我们用z4,z5,z6表示4,5,6的输出，经过softmax函数得到结果
![2248a8693da66a2c38cee8cf21c1c639.gif](/img/机器学习/softmax结果.gif)

要是用梯度下降，就需要有一个损失函数。这里用交叉熵损失函数
![98e6f99c9b449f8a89d39e34bb8e2d04.gif](/img/机器学习/交叉熵损失.gif)
其中，y代表真实值，a代表softmax求出的值。
其中假设真实值为y_j = 1，公式变为
![709c2acbda69c5e548ab1c3c67ce6ba7.gif](/img/机器学习/交叉熵损失简化.gif)
求导分析：
分为j=i和j!=i两种情况：
![abd6bd7e79a13f7b89b6200be356c563.gif](/img/机器学习/softmax求导结果.gif)
用这个结果，乘以交叉熵损失函数的求导
loss求导为 - 1/ai
则softmaxloss为：
![50b15ebc61acdf349c5a5d4dd8b29742.gif](/img/机器学习/softmaxloss.gif)
