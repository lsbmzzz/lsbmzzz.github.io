---
layout:     post
title:      CTR预估的一些方法
subtitle:   CTR预估的一些方法
date:       2019-08-24
author:     正版慕言
header-img: img/blog_bg_1.jpg
catalog: true
mathjax: true
tags:
    - 机器学习

---

> CTR(Click-Through-Rate,点击通过率)，指的是网络广告的点击到达率，即广告的实际点击次数除以广告的投放量，是衡量互联网广告效果的一个重要指标。

CTR的计算公式：$CTR= \frac{Click}{Show Content}$

在CTR预估中，高维度、稀疏是数据的典型特点

#### Embedding

Embedding是一个将离散变量转为连续向量表示的方式。它不光能够减少离散变量的空间维度，还能够有意义的表示该变量。

one-hot编码：在一个很大的向量里，只有一个索引为非零值。

对于高维度稀疏的one-hot编码，embedding学习一个低维度稠密的向量，将大型稀疏向量转化到保留语义关系的低维空间。



## CTR预估的流程

![CTR预估流程](/img/机器学习/CTR预估流程.jpg)

流程中主要包括离线和在线两部分，离线部分主要目标是训练出可用的模型，在线部分主要考虑模型上线后可能出现的性能随时间下降的问题。

1. 离线部分：数据的收集和预处理、数据集的构造和特征选择、模型和超参数的选择、A/B test
2. 在线部分：设定简单的规则过滤异常数据、根据在线收集的数据进行模型的更新、接收数据请求并返回预测结果


CTR预估的主流模型有以下几种：

## LR(logistic Regression)

LR模型是一个广义的线性模型，可以看做一个感知机模型。

$$
\mathbf &s = \mathbf{wx+b} \\
y $= \frac{1}{1 + e^{-s}}
$$

![感知机模型](/img/机器学习/感知机模型.jpg)

* LR模型的优点：简单、易于实现、可解释性强
* LR模型的缺点：由于线性模型本身的局限，不能处理特征和目标之间的非线性关系；特征的选择上非常耗费精力

为了学习到特征和目标之间的非线性关系，常用的方法有：

* 连续特征离散化：一般将原始的连续的值域划分成多个区间，变成分段的线性函数，这就相当于引入了非线性结构。
* 特征之间交叉：在CTR任务中，有时候单个特征对目标判定的贡献较弱，但多个特征组合就能产生较强的贡献。

## LR + GBDT(Facebook)

GBDT是多棵回归树组成的森林，后一棵树利用前面的结果和真实结果的残差作为你和目标，每个节点的分裂是一个特征选择过程，同时多层节点的结构进行了有效的特征组合，从而非常高效的解决了特征选择和特征组合问题。

GBDT : ![GBDT介绍](https://lsbmzzz.github.io/2019/08/24/GBDT%E5%92%8CXGBoost%E7%9A%84%E7%90%86%E8%A7%A3/)

LR + GBDT 用来解决二分类问题，预测给用户推送的广告会不会被点击。其中GBDT用来提取训练集特征作为新的训练数据，LR用作对新训练数据的分类器。

步骤：
1. GBDT先对原始数据做训练，得到一个二分类器。
2. GBDT训练好数据做预测时，输出的不是概率值，而是把计算出的概率值所属的叶子节点置1，其他叶子节点置0。如下图，对于有两棵决策树的GBDT，其输出为(0,1,0,0,1)。
![GBDT_LR模型结构](/img/机器学习/GBDT+LR的模型结构.png)
3. 新的训练数据构造完成后，和原始训练数据的label一起输入到LR分类器进行最终分类器的训练。
4. 数据经过GBDT后变得稀疏，并且可能会因为弱分类器和叶子节点个数的影响而纬度过大。因此，在LR层中可以用正则化来减少过拟合风险。FB的论文中采用的是L1正则化。

GBDT可以对连续值特征或值空间不大的特征进行分裂，其每棵树的叶子节点组成的特征向量连接起来称为LR输入的特征向量。发挥线性模型易于处理大规模稀疏数据的优势。

## 混合逻辑回归MLR(Alibaba)

MLR是在LR的基础上，先对样本进行聚类，然后再在聚类后的样本分片上应用LR进行CTR预估。

$$f(x) = \sum_{i=1}^m \pi_i(x) \cdot \eta_i(x) = \sum_{i=1}^m \frac{e^{\mu_i \cdot x}}{\Sigma_{j=1}^m e^{\mu_j\cdot x}} \cdot \frac{1}{1 + e^{-w_i \cdot x}} $$

如上式，先用聚类函数$\pi$对样本进行分类（softmax函数进行多分类），再用LR模型计算样本在分片中的CTR，然后将二者相乘，再求和。

超参数m=1时，退化为普通LR。当m越大，拟合能力越强。但随着m的增长，所需的训练集也要增长。（阿里给出m=12）

![MLR解决菱形分类面](/img/机器学习/MLR解决菱形分类面的样例.png)

MLR适用大规模稀疏数据场景。

MLR可以看做一个单隐层神经网络。

## FM和FFM

GBDT等基于树的模型的缺点：

1. 不适合学习高维度稀疏的特征组合
2. 不能学习到很少或没有出现过的特征组合

FM模型通过隐向量的内积提取特征组合，对训练数据中很少或没有出现过的特征组合也能够学习到。

因子分解机（FM）通过特征对之间的隐变量内积来提取特征组合：

$$y = w_0 + \sum_{i=1}^nw_ix_i + \sum_{i=1}^n\sum_{j=i+1}^n \left \langle v_i,v_j \right \rangle x_ix_j$$

对特征$i$和$j$来说，在没有成对出现过的情况下，可能都与特征$k$成对出现过，那么他们之间也有一定的相关性，这种相关性通过FM也能够学习到。

**FFM模型是FM模型的扩展，引入了field的概念。**

FFM把相同性质的特征归于同一个field。同理，商品的末级品类编码也可以放在同一个field中。

在FFM中每一维特征$x_i$针对其他特征的每一种field $f_i$都会学习一个隐向量$v_{i,f_i}$。因此隐向量不仅与特征相关，还与field相关。

假设n个特征属于f个field，那么FFN的二次项有nf个隐向量。

$$y = w_0 + \sum_{i=1}^nw_ix_i + \sum_{i=1}^n\sum_{j=i+1}^n \left \langle v_{i,f_j}, v_{j,f_i} \right \langle x_ix_j $$

FM可以看做FFM的特例，相当于把所有特征归到一个field时的FFM模型。

## 深度学习 DNN、CNN、RNN
