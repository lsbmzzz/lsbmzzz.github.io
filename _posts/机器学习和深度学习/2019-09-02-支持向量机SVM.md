---
layout:     post
title:      支持向量机SVM
subtitle:   支持向量机SVM
date:       2019-09-02
author:     正版慕言
header-img: img/blog_bg_1.jpg
catalog: true
mathjax: true
tags:
    - 机器学习

---

SVM是建立在统计学的VC维理论和结构风险最小化原理基础上的根据有限的样本信息在模型复杂性和学习能力之间寻找最佳折中来获得最好泛化能力的方法。

简单的理解，用二维空间上的二分类为例，SVM就是找到一条分割线将两类样本分开。如下图，这样的线可能有无数条，但那条是最优的，就是我们要考虑的问题。

![SVM](/img/MachineLearning/西瓜书-6.存在多个超平面.png)

直观上看，最优的划分超平面应该是正中间的那一个，因为该超平面对样本的扰动容忍性最好。

# 支持向量机的基本模型

在给定的样本空间$D=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{m},y_{m})\}$中，划分超平面可以通过如下的线性方程表述：

$$\mathbf w^T \mathbf x + b = 0 $$

其中$\mathbf w = (w_1, w_2,..., w_d) $为法向量，它和偏置b一起决定了超平面的位置。我们可以记为$(\mathbf w, b)$。

样本空间中的任意点到超平面的距离为

$$\begin{equation}
r=\frac{|w^{T}x+b|}{||w||}
\end{equation}$$

$$ \| \mathbf w\| $$为欧几里得范数

假设超平面能将样本正确分类，那么有$\mathbf w^T\mathbf x_i + b < 0 $令

$$
\left\{\begin{matrix}
w^{T}x_{i}+b\geq +1, & y_{i}=+1 \\ 
w^{T}x_{i}+b\leq -1, & y_{i}=-1
\end{matrix}\right.
$$

距离超平面最近的训练点使等号成立，它们叫做**支持向量**。

两个异类支持向量到超平面的距离之和称为**间隔**：

$$\begin{equation}
\gamma =\frac{2}{||w||}
\end{equation}$$

要找到具有最大间隔的超平面，就是要能找到让$\gamma$最大的$\mathbf w$和$b$。

$$\max_{\mathbf w, b}\frac{2}{\| \mathbf w\|} , y_i(\mathbf w^T\mathbf x_i + b) \geqslant 1 $$

可以看出，为了最大化间隔，只要最大化$\|\|\mathbf w\|\|^{-1} $，等价于最小化$\|\|\mathbf w\|\|^2$。因此上式可以转化为

$$\min_{\mathbf w, b}\frac{1}{2}\|\mathbf w\|^2 $$

这就是支持向量机的基本模型。


# 对偶问题

我们要通过求解

$$
\min_{\mathbf w, b}\frac{1}{2}\|\mathbf w\|^2 \ \ \ \ (1) \\
s.t. y_i(\mathbf w^T\mathbf x_i + b) \geqslant 1, i = 1, 2, ..., m
$$

来得到最大间隔划分超平面对应的模型

$$f(\mathbf x) = \mathbf w^T \mathbf x + b \ \ \ \ (2) $$ 

可以看到，我们的求解问题(1)本身是一个凸二次规划问题，可以用现成的优化计划包求解。但我们有更高效的方法。

我们对公式(1)使用拉格朗日乘子法得到它的“对偶问题”。具体地说就是对(1)的每一条约束，添加拉格朗日乘子$\alpha_i \geqslant 0 $。该问题的拉格朗日函数可以写成

$$L(\mathbf w, b, \alpha) = \frac{1}{2} \|\mathbf w\|^2 + \sum_{i = 1}^m \alpha_i(1 - y_i(\mathbf w^T\mathbf x_i + b)) \ \ \ \ (3) $$

其中，$$\alpha = (\alpha_1, alpha_2, ..., alpha_m) $$

令$L$对$\mathbf w$和$b$的偏导为0，可以得到

$$
\begin{align}
\mathbf w &= \sum_{i = 1}^m \alpha_iy_i\mathbf x_i \ \ \ \ (4) \\
0 &= \sum_{i = 1}^m\alpha_iy_i \ \ \ \ (5)
\end{align}
$$

将(4)代入(3)，就可以将其中的$\mathbf w, b $消去，再考虑(5)的约束，就可以得到(1)的对偶问题：

$$
\begin{align*}
\max_{\alpha} \sum_{i = 1}^m\alpha_i - \frac{1}{2}\sum_{i = 1}^m\sum_{j = 1}^m \alpha_i\alpha_jy_iy_j\mathbf x_i^T\mathbf x_j \ \ \ \ (6) \\
s. t. \sum_{i = 1}^m\alpha_iy_i = 0, \alpha_i \geqslant 0, i = 1, 2, ..., m
\end{align*}
$$

求解得到$\alpha$后，求出$\mathbf w, b$，就能够得到模型

$$
f(\mathbf x) = \mathbf w^T\mathbf x + b = \sum_{i = 1}^m\alpha_iy_i\mathbf x_i^T\mathbf x + b \ \ \ \ (7)
$$

从对偶问题(6)解出的$\alpha_i$是(3)中的拉格朗日乘子，它对应着训练样本$(\mathbf x_i, y_i)$。在(1)中右不等式约束，因此上述过程满足KKT条件，即要求

$$
\begin{align*} 
\left\{\begin{matrix}
\alpha_i \geqslant 0 \\ 
y_if(\mathbf x_i) - 1 \geqslant 0 \\ 
\alpha_i(y_if(\mathbf x_i) - 1) = 0
\end{matrix}\right. \ \ \ \ (8)
\end{align*}
$$

对于任意训练样本$(x_i, y_i)$，总有$\alpha_i = 0$或$y_if(\mathbf x_i) = 1 $。若$\alpha_i = 0$，则该样本不会在(7)的求和中出现，也就不会对$f(x)$有影响；若$\alpha_i > 0$，则必有$y_if(\mathbf x_i) = 1 $，对应的样本点位于最大间隔边界上，是一个支持向量。

这显示出了支持向量的一个重要性质：训练完成后，大部分训练样本不需要保留，最终的模型只和支持向量相关。


# SMO算法

SMO算法采用固定$\alpha_i$之外的所有参数求$\alpha_i$极值的方法求解。因为存在约束$\Sigma_{i = 1}^m\alpha_iy_i = 0$，因此固定$\alpha_i$之外的其它变量后，$\alpha_i$可以由其它变量导出。

SMO每次选择两个变量$\alpha_i, \alpha_j$，固定其他参数。这样，在参数初始化后，SMO不断执行如下步骤直到收敛：

* 选取一对更新变量$\alpha_i, \alpha_j$；
* 固定其他参数，求解式(6)获得更新后的$\alpha_i, \alpha_j$。

只要选取的$\alpha_i, \alpha_j$中有一个不满足KKT条件，目标函数就会在迭代后增大。KKT条件违背程度越大，变量更新后可能导致的目标函数值增幅就越大。所以，SMO每次先选择违背KKT条件最大的变量，第二个变量选择让目标函数值增长最快的变量。

由于比较各变量对应的目标函数值增幅的复杂度太高，因此SMO采用一种启发式：让选取的两个变量对应的样本之间的间隔最大（直观的看，这两个变量之间差别很大，所以对他们进行更新收益更高）。

SMO高效的原因在于仅优化两个函数可以做到非常高效。推导：

仅考虑$\alpha_i, \alpha_j$，那么式(6)中的约束可以重写为：

$$
\alpha_iy_i + \alpha_jy_j = c, \alpha_i \geq 0, \alpha_j \geq 0 \ \ \ \ (9)
$$

其中

$$
c = -\sum_{k \neq i,j}\alpha_ky_k
$$

是让$\Sigma_{i = 1}^m \alpha_iy_i = 0 $的常数

用(9)消去(6)中的变量$\alpha_j$，就得到一个关于$\alpha_i$的单变量二次规划问题，约束是$\alpha_i \geq 0$

这样的二次规划问题具有闭式解，不用调用值优化算法就能高效计算优化后的$\alpha_i$和$\alpha_j$

偏移项b的确定：

对任意支持向量$(x_s, y_s)$均有$y_sf(x_s) = 1$，即

$$
y_s(\sum_{s \in S}\alpha_iy_ix_i^Tx_s + b) = 1
$$

其中，S为所有支持向量的下标集

我们理论上可以采用上式来求解b，但在现实中我们可以采用一个更鲁棒的方法：使用所有支持向量求解的平均值

$$
b = \frac{1}{|S|}\sum_{s \in S}(1 / y_s - \sum_{i \in S}\alpha_iy_ix_i^Tx_s)
$$


# 核函数

当数据线性不可分时，SVM的处理方法是使用恰当的核函数将原始空间样本映射到一个更高维的空间，使样本在高维空间中线性可分。

SVM常用的核函数

| 名称 | 表达式 | 参数 |
| --- | --- | --- |
| 线性核 | $\kappa(x_i, x_j) = x_i^Tx_j$ |  |
| 多项式核 | $\kappa(x_i, x_j) = (x_i^Tx_j)^d $ | $d \geq 1$为多项式次数 |
| 高斯核 | $\kappa(x_i, x_j) = exp(-\frac{\|\|x_i - x_j\|\|^2}{2\sigma^2}) $ | $\sigma > 0$为高斯核的带宽 |
| 拉普拉斯核 | $\kappa(x_i, x_j) = exp(-\frac{\|\|x_i - x_j\|\|}{\sigma}) $ | $\sigma > 0$ |
| Sigmoid核 | $\kappa(x_i, x_j) = tanh(\beta x_i^Tx_j + \theta) $ | $\beta > 0, \theta < 0$ | 

核函数还可以组合得到。

如果$\kappa_1, \kappa_2$是核函数，则对于任意正数$\gamma_1, \gamma2$，其线性组合

$$\gamma_1\kappa_1 + \gamma_2\kappa_2$$

也是核函数。

如果$\kappa_1, \kappa_2$是核函数，则他们的直积

$$\kappa_1 \bigotimes \kappa_2(x, z) = \kappa_1(x, z)\kappa_2(x, z) $$

也是核函数。

如果$\kappa_1$是核函数，则对于任意函数$g(x)$

$$\kappa(x,z) = g(x)\kappa_1(x,z)g(z) $$

也是核函数。

在数据线性可分时，线性核可以有效处理。当数据线性不可分时，就通过上面的其他核函数代替公式(6)中的线性核函数，从而将原始样本映射到一个更高维度的空间而线性可分。

核函数的选取：

1. 根据先验知识
2. 采用交叉验证


# 软间隔和正则化

前面介绍的支持向量机要求所有样本都要满足约束，称为**硬间隔**。而**软间隔**允许有些样本不满足约束

$$
y_i(\mathbf w^T\mathbf x_i + b) /geq 1
$$

从而降低噪声的影响。当然，在最大化间隔时，要让不满足约束的样本尽可能少。

我们引入一个松弛变量$\phi_i \geq 0$来允许错误的发生，也就是允许出现间隔小于1的点：

$$
y_i(w^Tx_i + b) \geq 1 - \phi_i
$$

同时我们要限制这种松弛，因此需要在目标函数上加一个惩罚函数

$$\min_{\mathbf w, b, \phi_i} \frac{1}{2}\|\mathbf w\|^2 + C\sum_{i = 1}^m\phi_i $$

上式中C为乘法因子，C越大表示离群点越不被希望出现。当C无穷大时，等价于硬间隔SVM；当C取有限值时，允许有一些样本不满足约束。

加入惩罚项后，SVM的对偶问题就变成了

$$
\begin{align*}
\max_{\alpha}\sum_{i = 1}^m\alpha_i - \frac{1}{2}\sum_{i = 1}^m\sum_{j = 1}^m\alpha_i\alpha_jy_iy_j\mathbf x_i^T\mathbf x_j \\
s.t. \ \sum_{i = 1}^m\alpha_iy_i = 0 \\ 
0 \leq \alpha_i \leq C, i = 1,2,...,m
\end{align*}
$$

引入软间隔后，$\alpha$的范围变成了0到C。

对于软间隔支持向量机，KKT条件要求

$$
\left\{\begin{matrix}
\alpha_i \geq 0, \mu_i \geq 0 \\ 
y_if(\mathbf x_i) - 1 + \phi_i \geq 0 \\ 
\alpha_i(y_if(\mathbf x_i) - 1 + \phi_i) \geq 0 \\ 
\phi_i \geq 0, \mu_i\phi_i \geq 0
\end{matrix}\right.
$$
