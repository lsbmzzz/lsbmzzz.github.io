---
layout:     post
title:      Bagging和Boosting
subtitle:   Bagging和Boosting
date:       2019-09-02
author:     正版慕言
header-img: img/blog_bg_1.jpg
catalog: true
mathjax: true
tags:
    - 机器学习

---

Boosting和Bagging都是将已有的分类器或回归器组合起来，形成一个更强大的分类器或回归器。

# Bootstrap 自助法

**自助法** 是一种有放回的抽样方法。

最常用的一种是 .632 自助法，对于一个包含D个样本的数据集，有放回的抽样D次，每个样本被选中的概率是 $ \frac{1}{D} $ ，未被选中的概率就是$1 - \frac{1}{D} $，这样一个样本在抽取出的训练集中未出现的概率就是$(1 - \frac{1}{D})^D $，这一概率趋近于$e^{-1} = 0.368 $，所以训练集中的 样本大约占原来数据集的63.2%。

# Bagging

1. 从原始样本集中获取训练集。采用自助法获取k个训练集。
2. 每次使用1个训练集训练出一个模型，得到k个模型。
3. 对分类问题：投票法获得结果；对回归问题：计算均值获得结果。

# Boosting

Boosting的主要思想是将一些弱分类器组合成一个强分类器。在PAC框架下，一定能够将弱分类器组合成强分类器。

Boosting的核心问题：

1. 每一轮改变训练数据的权值，降低分类正确的样例的权值，提高分类错误的样例的权值。
2. 弱分类器的组合方式：AdaBoost使用加法模型，通过加权多数表决的方式进行线性组合；提升树通过拟合残差的方式来逐步减小残差，叠加每一步生成的模型。

# Bagging和Boosting的区别

1. 样本选择上：
Bagging每一个分类器的训练集是用自助法抽取的，各训练集之间是独立的。
Boosting每一轮的训练集不变，只是样本的权重发生变化，这些权重是根据上一轮的结果来调整的。

2. 样本权重
Bagging的样本权重是均匀的
Boosting的样本权重是不断调整的，分类错误的样本权重会增加，正确的样本权重会减少

3. 组合
Bagging所有的弱分类器权重相等
Boosting每个弱分类器都有自己的权重，分类误差小的分类器权重会更大。

4. 并行计算
Bagging的所有分类器可以并行生成
Boosting的各个分类器只能顺序生成，因为每个模型都需要前一轮模型的效果。

# 与决策树组合得到的新算法

Bagging + 决策树 = 随机森林RF
AdaBoost + 决策树 = 提升树
Gradient Boosting + 决策树 = GBDT