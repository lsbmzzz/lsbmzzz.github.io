---
layout:     post
title:      BP神经网络原理
subtitle:   BP神经网络原理
date:       2019-08-21
author:     正版慕言
header-img: img/blog_bg_1.jpg
catalog: true
mathjax: true
tags:
    - 机器学习
    - 深度学习
    - BP神经网络

---

> BP，Back Propagation 

![590fa20f4fcede1b5fa2141563dc337c.png](/img/Journal/NeuralNetworks/一个单隐层BP网络.png)
（图中未包含bias）

这个网络的工作原理：

#### 前向传播
1. 输入层接收一组输入$x_1, x_2, ..., x_m$
2. 输入层通过与隐层的连接的权重，产生一组输出$s_1, s_2, ..., s_n$作为隐层的输入
3. 通过隐层节点的θ激活函数输出$\theta(s_j)$
4. 将这些输出通过隐层与输出层的连接权重，产生输出层的输入
5. 通过输出层的激活函数θ，输出$\overline y_j$

#### 反向传播
输入数据是已知的，那么影像输出的就是那些连接的权重了。反向传播的任务就是调整这些连接权重，让它们向正确的方向变化。
对于给定的训练样本，我们已经知道正确结果，那么从输入经过网络得到的输出与正确结果之间会有一个误差。将这个误差降低到最小，就是输出结果接近了正确结果。怎么让误差最小呢？

**写出误差表达式**。这里使用最小化均方根误差（使函数连续可导），定义损失函数：

$$L(e) = \frac{1}{2}\sum_{j=0}^ke_j^2 = \frac{1}{2}\sum_{j=0}^k(\overline y_j-y_j)^2$$

**怎么最小化L**？可以采用随机梯度下降SGD。对每个训练样本都使权重往其负梯度方向变化。
1. 用$w_{ij}^1, w_{ij}^2, s_j^1, s_j^2$分别表示输入层第i个节点到隐层第j个节点的权重、隐层第i个节点到输出层第j个节点的权重、隐层第j个几点的输入、输出层第j个节点的输入。（用上标表示层数）
2. 基于这种表示，有

$$\frac{\partial L}{\partial w_{ij}^1} = \frac{\partial L}{\partial s_j^1}\cdot \frac{\partial s_j^1}{\partial w_{ij}^1}$$

由于有

$$s_j^1 = \sum_{i=1}^mx_i \cdot w_{ij}^1 $$

故

$$\frac{\partial s_j^1}{\partial w_{ij}^1} = x_i $$

因此有

$$\frac{\partial L}{\partial w_{ij}^1} = x_i \cdot \frac{\partial L}{\partial s_j^1} $$

3. 此时只要计算L对s的偏导即可。由于s对所有输出层都有影响，所以有

$$\frac{\partial L}{\partial s_j^1}=\sum_{i=1}^k\frac{\partial L}{\partial s_i^2} \cdot \frac{\partial s_i^2}{\partial s_j^1} $$

其中$s_i^2$可以由隐层的输出和权重得到：

$$s_i^2=\sum_{j=0}^n\theta(s_j^1)\ast w_{ji}^2 $$

因此有

$$
\begin{align*}
\frac{\partial s_i^2}{\partial s_j^1}&=\frac{\partial s_i^2}{\partial \theta(s_j^1)} \cdot \frac{\partial \theta(s_j^1)}{\partial s_j^1}=w_{ji}^2\cdot \theta '(s_j^1) \\
\frac{\partial L}{\partial s_j61}&=\sum_{i=1}^k\frac{\partial L}{\partial s_i^2} \cdot w_{ji}^2 \cdot \theta '(s_j^1) \\
&=\theta '(s_j^1)\cdot w_{ji}^2\cdot \sum_{i=1}^k\frac{\partial L}{\partial s_i^2}
\end{align*}
$$

令$\delta_i^l=\frac{\partial L}{\partial s_i^l}$，有

$$
\begin{align*}
\delta_j^1 &= \theta '(s_j^1) \cdot \sum_{i=1}^k\delta_i^2\cdot w_{ji}^2 \\
\delta_i^2 &= \frac{\partial L}{\partial s_i^2} = \frac{\partial \Sigma_{j=0}^k\frac{1}{2}(\overline y_j - y_j)^2}{\partial s_i^2}
&= (\overline y_i - y_i)\cdot \frac{\partial \overline y_i}{\partial s_i^2}
&= e_i \cdot \theta '(s_i^2)
\end{align*}
$$

这里可以看到，反向传播的就是误差e。

反向传播的过程：输出层的每个节点都会得到一个误差e，把e作为输出层的反向输入（相当于输出层当成输入层）把e向回传播，先得到输出层的δ，然后根据连接权重w向隐层传输，就得到隐层

$$\delta_j^1=\theta '(s_j^1)\cdot \sum_{i=1}^k \delta_i^2 \cdot w_{ji}^2 $$

第一层的权重梯度：

$$\frac{\partial L}{\partial w_{ij}^1}=x_i \cdot \theta_j^1$$

第二层权重梯度：

$$\frac{\partial L}{\partial w_{ij}^2} = \frac{\partial L}{\partial s_j^2} \cdot \frac{\partial s_j^2}{\partial w_{ij}^2} = \delta_j^2 \cdot \theta(s_i^1) $$

可以看到，**每个权重的梯度都等于与其相连的前一层节点的输出乘以与其相连的后一层的反向传播的输出**。
反向传播得到所有的δ以后，就可以更新权重了。
下图是一个BP神经网络工作过程
![7e27959b73b46780dfb3a635fa1d188c.png](/img/Journal/NeuralNetworks/BP神经网络工作过程.png)
图中，每一个节点的输出都和权重矩阵中对应同一列（行）的元素相乘，然后同一行（列）累加，作为下一层对应节点的输入。
单个节点的抽象示意图：
![714237a8ea053b9bea599632ddb42871.png](/img/Journal/NeuralNetworks/单个节点的抽象.png)

#### 一些常用的激活函数
激活函数θ可以使用的有Sigmoid、tanh、ReLU、Leaky ReLU等。
$$
\begin{align*}
Sigmoid &: f(x) = \frac{1}{1 + e^{-x}} \\
tanh &: tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \\
ReLU &: relu(x) = max(0, x) \\
Leaky ReLU &: f(x) = max(\alpha x, x)
\end{align*}
$$