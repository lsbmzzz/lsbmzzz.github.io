---
layout:     post
title:      LR回归
subtitle:   LR回归
date:       2019-09-24
author:     正版慕言
header-img: img/blog_bg_1.jpg
catalog: true
mathjax: true
tags:
    - LR
    - 机器学习

---


训练集：

$$
D = {(x^1, y^1), (x^2, y^2), ..., (x^N, y^N)}
$$

其中x为m维向量，y为分类结果

$$
x^i = [x_1^i, x_2^i, ..., x_m^i], y \in {0, 1}
$$

上述问题可以简化为找到一个决策函数$y^{\ast}=f(x)$ 使其在未知数据集上有较好的表现。

sigmoid函数：

$$
g(x) = \frac{1}{1 + e^{-x}}
$$

![sigmoid函数](/img/Journal/CTR/sigmoid函数.png)

#### 决策函数

LR的假设是

$$
P(y=1:x;\theta) = g(\theta^Tx) = \frac{1}{1 + e^{-\theta^T\ast x}}
$$

相应的，决策函数为

$$
y^{\ast} = 1, if \ P(y=1|x)>0.5
$$

阈值取0.5是一般的做法

#### 参数求解

统计学中常用的方法是最大似然估计。找到一组参数，使在这组参数下我们的数据似然度最大。

在LR模型中，似然度可以表示为

$$
L(\theta)=P(D|\theta) = \prod P(y|x;\theta) = \prod g(\theta^Tx)^y(1-g(\theta^Tx))^(1-y)
$$

取对数得对数似然度

$$
l(\theta) = \sum y log g(\theta^Tx) + (1-y)log(1-g(\theta^Tx))
$$

损失函数使用log损失：

$$
-ylogp(y|x)-(1-ylog(1-p(y|x)))
$$

取数据集上的平均log损失，有

$$
J(\theta) = -\frac{1}{N}l(\theta)
$$

(在LR中，最大化似然函数和最小化损失是等价的)

该优化问题可以用梯度下降来求解。

1. 选择下降方向（梯度方向$\triangledown j(\theta)$）
2. 选择步长，更新参数$\theta^i=\theta^{i-1}-\alpha^i\triangledown J(\theta^{i-1}) $
3. 重复以上步骤直到满足条件

损失函数的梯度计算方法为：

$$
\frac{\partial J}{\partial \theta} = -\frac{1}{n}\sum_i(y_i-y_i^{\ast})x_i+\lambda\theta
$$

#### 正则化

[L1, L2正则化](https://lsbmzzz.github.io/2019/08/26/L1-L2%E6%AD%A3%E5%88%99%E5%8C%96/)