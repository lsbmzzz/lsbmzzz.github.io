---
layout:     post
title:      决策树算法：ID3、C4.5、CART
subtitle:   决策树算法：ID3、C4.5、CART
date:       2019-09-24
author:     正版慕言
header-img: img/blog_bg_1.jpg
catalog: true
mathjax: true
tags:
    - 决策树

---

#### 决策树

一般的，一棵决策树包含一个根节点、若干内部节点和若干叶节点。

决策树的生成是一个递归的过程，三种情况会导致递归返回：
1. 当前节点包含的样本全部属于同一类别无需划分
2. 当前属性集为空，或所有样本在所有属性上取值相同，无法划分（叶节点）
3. 当前节点包含的样本集合为空，不能划分（叶节点）

#### ID3决策树

用信息增益来决定对集合D的划分

$$
\begin{align}
Ent(D) = -\sum_{k=1}^{|y|}p_klog_2p_k \\
Gain(D, a) = Ent(D) - \sum_{v = 1}^V\frac{|D^v|}{|D|}Ent(D^v)
\end{align}
$$

缺点是可能对一些属性过度划分

#### C4.5决策树

C4.5决策树不直接使用信息增益作为划分样本的主要依据，而是使用增益率

$$
\begin{align}
Gain_{raito}(D, a) = \frac{Gain(D, a)}{IV(a)} \\
IV(a) = -\sum_{v=1}^V\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}
\end{align}
$$

C4.5使用的增益率可能对取值数目较少的属性有所偏好

#### CART决策树(Classification and Regression Tree)

CART使用基尼系数来划分

基尼值和基尼系数

$$
\begin{align}
Gini(D) = \sum_{k=1}^{|y|}\sum_{k'}p_kp_k' = 1 - \sum_{k=1}^{|y|}p_k^2
Gini_{index}(D, a) = \sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)
\end{align}
$$

在候选属性中选择基尼系数小的来优先划分。