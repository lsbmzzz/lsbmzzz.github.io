---
layout:     post
title:      机器学习EM算法
subtitle:   机器学习EM算法
date:       2019-08-26
author:     正版慕言
header-img: img/blog_bg_1.jpg
catalog: true
mathjax: true
tags:
    - 机器学习
    - EM算法

---

> EM(Expectation-Maximun)算法中文为期望最大化算法，是一种通过迭代来进行极大似然估计的优化算法。

## EM算法的一些基础知识

#### 最大似然估计

举例：调查学校男生和女生的身高分布。假设随便找了100个男生，和100个女生。按照性别划分两组，先统计100个男生的身高。假设他们的身高服从高斯分布，但均值和方差$\mu, \sigma^2$都不知道，那么我们要估计的参数就是$\theta = [\mu,\sigma^2]^T$。

如果我们知道了样本集$X={x_1,x_2,...,x_N} $，这些样本独立同分布；概率密度$p(x_i\|\theta) $为抽到男生$i$的概率，那么同时抽到这些样本的概率就是各自概率的乘积：

$$L(\theta) = L(x_1,x_2,...,x_N;\theta) = \prod_{i=1}^Np(x_i,\theta), \theta \in \Theta $$

上式表示概率密度函数的参数是$\theta$时得到这组样本的概率。我们要找到一个参数$\theta$让其对应的似然函数$L(\theta)$最大，这就是$\theta$的最大似然估计量，记为$\hat \theta = argmax \mathcal l(\theta)$

#### 求最大似然估计值的步骤

1. 写出似然函数  
$$L(\theta) = L(x_1,x_2,...,x_N;\theta) = \prod_{i=1}^Np(x_i;\theta), \theta \in \Theta $$  
2. 对似然函数取对数  
$$H(\theta) = lnL(\theta) = ln\prod_{i=1}^Np(x_i;\theta) = \sum_{i=1}^Nlnp(x_i;\theta) $$  
3. 求导令导数为0，得到似然方程
4. 解得似然方程的结果

#### Jensen不等式

Jensen不等式：如果函数f是凸函数，x是随机变量，那么$E[f(x)] \geqslant f(E[x]) $。当且仅当x为常量时，取等号。

## EM算法

在上面的例子中，如果我们不知道抽取的200人中，每一个人是男生还是女生，这时候对每一个样本就有两个值需要估计：

1. 这个人是男生还是女生
2. 男生和女生分别对应的高斯分布参数

**EM算法要解决的问题就是：**

1. 每个样本属于哪一个分布
2. 每一个分布对应的参数

#### 隐变量

现实应用中会遇到训练样本中属性值不完整的情况，不完整的变量称为“隐变量”。可以用$\mathbf X$表示观测到的变量集，$\mathbf Z$表示隐变量集，$\Theta$表示模型参数。

如果Z已知，就可以用最大似然估计来求解了。所以，我们想办法把Z变成已知的。

在上面的例子中，使用EM算法求解的步骤为：

1. 初始化身高分布的参数
2. 计算每一个人更可能属于男生还是女生
3. 通过分为男生和女生的人，分别用最大似然估计来重新估计男生身高分布的参数，更新分布
4. 重复1到3，直到分布参数不再变化。

#### EM算法的过程

E步：根据参数初始值或上一步迭代计算出的值（也就是隐变量的期望），作为隐变量当前的估计值  
$$Q_i(z^{(i)}) = p(z^(i) \| x(i);\theta) $$

M步：将似然函数最大化，以获得新的参数值  
$$\theta = arg\max_\theta \sum_i\sum_{z^(i)}Q_i(z^{(i)})log\frac{p(x_{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} $$

#### EM算法推导

