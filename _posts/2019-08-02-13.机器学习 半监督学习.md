---
layout:     post
title:      13.机器学习|半监督学习
subtitle:   机器学习周志华 学习笔记————机器学习|半监督学习
date:       2019-08-02
author:     正版慕言
header-img: img/blog_bg_3.jpg
catalog: true
tags:
    - 机器学习
    - 西瓜书

---

#### 半监督学习

在现实中，我们往往能容易的收集到大量的未标记样本，而获取标记却很困难。
让学习器不依赖外界交互、自动利用未标记样本来提升学习性能，就是**半监督学习**。

要利用未标记样本，必然要做一些将未标记样本揭示的数据分布信息与类别标记相联系的假设。
常见的是“**聚类假设**”：假设数据存在簇结构，同一个簇的样本属于同一个类别。下图就是基于聚类假设来利用未标记样本：
![西瓜书-13.聚类假设.png](/img/西瓜书-13.聚类假设.png)

另一种常见的假设是“**流形假设**”：假设数据分布在一个流形结构上，临近的样本拥有相似的输出值。它可以看做聚类的推广，但它对输出值没有限制，因此适用范围更广。

两种假设在本质上都是“**相似的样本拥有相似的输出**”。

半监督学习可以进一步划分为**纯半监督学习**和**直推学习**。前者假定训练数据中的未标记样本并非待预测的数据，后者假定学习过程中所考虑的未标记样本就是待遇测数据，学习目的是在这些未标记样本上获得最好的泛化性能。
![西瓜书-13.主动学习、纯半监督学习与直推学习.png](/img/西瓜书-13.主动学习、纯半监督学习与直推学习.png)

#### 生成式方法
生成式方法是直接基于生成式模型的方法。它假设所有数据（无论是否有标记）都是由同一个潜在的模型“生成”的，这个假设让我们能通过潜在模型的参数将未标记数据和学习目标联系起来，未标记数据的标记可以看做潜在模型的缺失参数。通常，可以基于EM算法进行极大似然估计求解。
不同的模型假设将产生不同的方法。

生成式方法简单、易实现，在有标记数据较少的情况下往往能取得比其它方法更好的性能。但是此方法的关键在于模型假设必须准确，否则未标记数据反而会降低模型的泛化性能。在现实任务中，除非拥有充分可靠的领域知识，否则很难事先做出准确的模型假设。

#### 半监督SVM
半监督支持向量机（Semi-Supervised Support Vector Machine, S3VM）是SVM在半监督学习上的推广。它试图找到能将两类有标记的样本分开，并穿过数据低密度区域的划分超平面。显然，这是聚类假设在考虑了线性超平面划分后的推广。
![西瓜书-13.S3VM.png](/img/西瓜书-13.S3VM.png)

S3VM最著名的是TSVM算法，它也是针对二分类问题的学习方法。
TSVM考虑对未标记样本进行各种可能的标记指派，然后再所有结果中寻找一个在所有样本上间隔最大化的划分超平面，此时的未标记样本的指派就是其预测结果。

给定已标记的Dl和为标记的Du，有l + u = m, TSVM的学习目标是为Du中的样本给出预测标记使
![西瓜书-13.TSVM.gif](/img/西瓜书-13.TSVM.gif)

显然标记指派是一个穷举过程。在一般情况下，必须考虑更高效的优化策略。

TSVM采用局部搜索来迭代寻找上式的近似解：

1. 利用标记样本学得一个SVM
2. 用这个SVM对未标记样本进行标记指派（伪标记），基于上面公式求解出新的划分平面和松弛向量
3. 此时的伪标记很可能不准确，因此Cu要小于Cl，使有标记样本权重更大。接下来，要找出两个标记指派为异类且很可能发生错误的未标记样本交换其伪标记，重新基于上面的公式求划分超平面和松弛向量。
4. 循环上面的第3步骤，逐渐增大Cu直到Cu = Cl。
![西瓜书-13.TSVM算法.png](/img/西瓜书-13.TSVM算法.png)

搜寻标记指派可能出错的每一对样本并进行调整，会带来巨大的计算开销。因此，S3VM研究的**一个重点是如何设计出高效的优化求解策略**。当前已发展出了一些方法，如**基于图核函数梯度下降的LDS、基于标记均值估计的meanS3VM**等。

#### 图半监督学习
我们可以将数据集映射为一个图。每个样本对应一个节点，若两个样本相似度高，则对应节点之间存在一条边，边的强度正比于样本相似度。有标记的样本认为染过色，无标记样本尚未染色。由此，半监督学习就可以认为是“颜色”在图上扩散的过程。

* 基于D构建一个图G = (V,E)，V为节点集，E为边集。
* 根据数据集中的节点相似度，用高斯函数计算亲和矩阵。高斯函数：
![西瓜书-13.高斯函数.gif](/img/西瓜书-13.高斯函数.gif)
* 假定从图中学得一个实值函数f:V→**R**，则可以定义关于f的“能量函数”
![西瓜书-13.E(f).gif](/img/西瓜书-13.E(f).gif)
E(f)为任意两个节点的预测结果的差值乘他们之间的权重wij的和。
* 能量函数对fu求偏导，得到未标注数据的预测结果

图半监督学习的优缺点：

* 优点：概念清晰，易于通过对所涉矩阵运算的分析来探索算法的性质
* 缺点：存储开销大，且由于构图过程仅能考虑训练集样本，当接收新样本时要么将其加入原数据集进行重构并重新进行标记传播，要么引入额外的预测机制。

#### 基于分歧的方法
基于分歧的方法使用多学习器，学习器之间的“分歧”对未标记数据的利用至关重要。

“**协同训练**”是此类方法的重要代表。

>**多视图**：一个数据对象往往同时拥有多个“属性集”，每个属性集构成一个“视图”。拥有多个视图的数据称为多视图数据。
>多视图具有“**相融互补性**”

协同训练利用了多视图的“相融互补性”，假设数据有两个充分且条件独立的视图，则协同训练的基本逻辑如下：

1. 在每个视图上基于有标记样本分别训练出一个分类器
2. 让每个分类器分别挑选自己“最有把握”的未标记样本赋予伪标记
3. 将伪标记样本提供给另一个分类器作为新增有标记样本用于训练更新
4. 重复2、3步骤直到两个分类器都不再发生变化

![西瓜书-13.协同训练算法.png](/img/西瓜书-13.协同训练算法.png)

协同训练的过程虽然简单，但理论证明若两个视图充分且条件独立，则可以利用未标记样本进行协同训练将弱分类器的泛化性能提升到任意高。不过视图的条件独立性在现实任务中通常难以满足，因此性能提升不会那么大。
基于分歧的方法需要能生成具有显著分歧、性能尚可的多个学习器，但当样本较少尤其是数据不具有多视图时，要做到这一点并不容易。

#### 半监督聚类
在现实聚类任务中我们往往能获得一些额外的监督信息，可以通过半监督聚类利用监督信息获得更好的效果。
聚类任务重获得的监督信息大致有两种类型：第一种是“必连”与“勿连”约束，第二种是少量有标记样本。

对于“必连”与“勿连”约束，可以使用“约束k均值”算法。它是k均值算法的扩展，在聚类过程中保持必连和勿连约束得以满足，否则会返回错误。
![西瓜书-13.约束k均值算法.png](/img/西瓜书-13.约束k均值算法.png)

对于有少量标记样本的情况，可以直接把标记样本作为种子，用他们初始化k均值算法的k个聚类中心，并在聚类簇迭代更新过程中不改变种子样本的簇隶属关系，就得到了约束种子k均值算法。
![西瓜书-13.约束种子k均值算法.png](/img/西瓜书-13.约束种子k均值算法.png)
