---
layout:     post
title:      AdaBoost算法原理
subtitle:   AdaBoost算法原理
date:       2019-08-25
author:     正版慕言
header-img: img/blog_bg_1.jpg
catalog: true
mathjax: true
tags:
    - 机器学习
    - AdaBoost

---

## 概率近似正确(probably approximately correct, PAC)

由Kearns和Valiant首先提出：

1. 一个概念如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么这个概念是强可学习的。
2. 一个概念如果存在一个多项式的学习算法能够学习它，但学习的正确率仅比随机略高，那么这个概念是弱可学习的。

Schapire后来证明在PAC学习的框架下，一个概念是强可学习的充要条件是它是弱可学习的。这样一来，在训练算法建模时如果发现了弱可学习算法，就能够将其提升为强可学习算法。

## 提升方法的关键要素和策略

提升方法的关键要素：

1. 在每一轮如何改变训练数据的权值或概率分布，修改的策略是什么？
2. 如何将弱分类器组合成一个强分类器？

AdaBoost的策略：

1. 提高被前一轮弱分类器错误分类的样本的权值，而降低被正确分类样本的权值。
2. 对弱分类器的组合，采取加权多数表决的方法：加大分类误差率小的弱分类器的权值，减小分类误差大的弱分类器的权值。

AdaBoost算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法的二分类学习算法。

## 前向分步算法

### 加法模型

加法模型是一种线性模型公式为：

$$f(x) = \sum_{m=1}^M\beta_mb(x;\gamma_m) $$

其中$b(x;\gamma_m)$是基函数，$\gamma_m$是基函数的参数，$\beta_m$是基函数的权重。

给定训练数据和损失函数，学习加法模型就成为了经验风险极小化问题：

$$min_{\beta_m,\gamma_m}\sum_{i=1}^NL(y_i, \sum_{m=1}^M\beta_mb(x;\gamma_m)) $$

上式即同时考虑N个样本在整个线性模型组中的损失函数的极小值。这是一个复杂的优化问题。

使用前向分步算法来求解这一优化问题的思路是：如果能够从前向后每一步只学习一个基函数及其系数，逐步逼近上式，那么就能极大的简化优化的复杂度。每一步的优化下面的损失函数：

$$min_{\beta_m, \gamma_m}\sum_{i=1}^NL(y_i, \beta_mb(x;\gamma_m)) $$

这样每次只要考虑一个基函数及其系数。

前向分步算法和贝叶斯估计的相同点：

1. 都假设每一步之间的基函数和系数是独立的(贝叶斯估计中称为独立同分布)
2. 这种假设会丢失一部分精度，因为每一步之间的依赖关系会被忽略

前向分步算法的策略也是经验风险最小化。

### 前向分步算法

给定训练集$T = (x_1, y_1), (x_2, y_2), ..., (x_N, y_N), x_i \in \mathcal X \subseteq R^n, y_i \in Y = {-1,1} $，损失函数$L(y,f(x))$，基函数的集合$b(x;\gamma) $，学习加法模型$f(x)$的前向分步算法如下：

1. 初始化$f_0(x) = 0$
2. 对于$m = 1,2,3,...,M$(M为基函数的个数)，有
	1. 在上一步得到的最优基函数的基础上，最小化本次单个基函数的损失函数：
	$(\beta_m, \gamma_m) = argmin_{\beta, \gamma} \sum_{i=1}^NL(y_i, f_{m-1}(x_i) + \beta_mb(x:\gamma_m)) $，得到本轮的最优基函数的参数$\beta_m, \gamma_m$。
	2. 更新$f_m(x) = f_{m-1}(x) + \beta_mb(x;\gamma_m) $
3. 得到最终的加法模型$f(x) = f_M(x) = \sum_{m=1}^M\beta_mb(x;\gamma_m) $。

前向分步算法将同时求解从m=1到M的所有参数$\beta,\gamma$的问题简化为逐次求解的局部最优化问题。

## AdaBoost算法

给定训练集$T = (x_1, y_1), (x_2, y_2), ..., (x_N, y_N), x_i \in \mathcal X \subseteq R^n, y_i \in Y = {-1,1} $，AdaBoost算法的步骤如下：

1. 初始化训练数据的权值分布(N为样本数量) 

$D_1 = (w_{11}, w_{12}, ..., w_{1N}), i = 1,2,...,N $

等概率分布(等概率分布体现了最大熵原理，在没有任何先验知识的情况下，等概率是最合理的)，让每个样本的权值相同，保证第一步在原始数据上学习基本分类器$G_1(x)$。
2. 假设总共训练M轮，则对于$m = 1,2,...,M$：
	1. 使用带权值分布$D_m$的训练集学习，得到本轮的基本分类器

	$$G_m(x):\mathcal X \rightarrow {-1,+1}$$

	2. 计算$G_m(x)$在本轮训练集上的分类误差率：

	$$e_m = \sum_{i=1}^NP(G_m(x_i) \neq y_i) =  \sum_{i=1}^Nw_{mi}I(G_m(x_i)\neq y_i) $$

	$e_m$是本轮被误分类的样本的权值之和，而与弱分类器内部无关。
	3. 根据$e_m$计算$G_m(x)$的模型系数：

	$$\alpha_m = \frac{1}{2}log\frac{1 - e^m}{e^m} $$

	代表本轮得到的弱分类器的重要程度(当$e_m\leqslant \frac{1}{2} $时，$\alpha_m \geqslant 0 $。并且$\alpha_m$随着$e_m$的减小而增大)。
	4. 更新下一轮训练集的权值分布：

	$$D_{m+1} = (w_{m+1,1}, ..., w_{m+1,i}, ..., w_{m+1,N}) $$

	其中

	$$w_{m+1,i} = \frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i)),i=1,2,...,N $$

	其中

	$$Z_m = \sum_{i=1}^Nw_{mi}exp(-\alpha_my_iG_m(x_i)) $$

	是规范化因子，使$D_{m+1}$称为一个概率分布。$w_{m=1,i}$也可以写成

	$$w_{m+1,i} = \left\{\begin{matrix}
	\frac{w_{mi}}{Z_m}e^{-\alpha_m}, & G_m(x_i) = y_i \\ 
	\frac{w_{mi}}{Z_m}e^{\alpha_m}, & G_m(x_i) \neq y_i
	\end{matrix}\right. $$

	可以看出，被基分类器分类错误的样本权值被扩大，而分类正确的样本权值减小。
3. 构建基本分类器的线性组合：

$$f(x) = \sum_{m=1}^M\alpha_mG_m(x) $$

得到最终的分类器

$$G(x) = sign(f(x)) = sign(\sum_{m=1}^M\alpha_mG_m(x)) $$