---
layout:     post
title:      L1、L2正则化
subtitle:   L1、L2正则化
date:       2019-08-26
author:     正版慕言
header-img: img/blog_bg_1.jpg
catalog: true
mathjax: true
tags:
    - 正则化
    - 机器学习

---

## L1、L2范数

范数衡量的是一个向量空间中每个向量的长度或大小。范数的定义：

$$ \| x \|_p = (\sum_{i=1}^n|x_i |^p)^{\frac{1}{p}} $$

当$p=1$时，就是L1范数：

$$ \|x \| = \sum_{i=1}^n|x_i | $$

当$p=2$时，就是L2范数：

$$ \|x \|_2 = (\sum_{i=1}^n|x_i |^2)^{\frac{1}{2}} $$



## L1和L2正则化

正则化方法是在原目标函数中加入惩罚项，来控制模型的复杂度：

$$\tilde J(w;X,y) = J(w;X,y) + \alpha\Omega(w) $$

上面的$\Omega$即为惩罚项，$\alpha$控制正则化强弱。

不同的$\Omega$函数对权重$w$的最优解有不同的偏好，所以会产生不同的正则化效果。常用的$\Omega$函数有两种：$L1$正则化和$L2$正则化。

$$
\begin{align*}
L1 : \Omega(w) &= \| w \|_1 = \sum_i | w_i | \\
L2 : \Omega(w) &= \| w \|_2^2 = \sum_i w_i^2
\end{align*}
$$

## 为什么L1、L2正则化能够解决过拟合问题

正则化通过降低模型的复杂性达到避免模型过拟合的目的。

#### 从结构风险最小化角度解释

结构风险最小化指的是在经验风险最小化的基础上，尽可能采用简单的模型来提高泛化效果。

![L1L2正则化图像表示](/img/Journal/L1L2正则化图像表示.jpg)

上图分别为对于一个$w$是二维的问题，使用L1和L2正则化的目标函数求解的示意图。其中在同颜色的等高线上，每一组$w1,w2$代入的值都相同。

在不加入正则化的时候，我们获得的最终结果将是最内侧等高线上的点；

加入L1正则化后（在图中画出$ | w_1 | + | w_2 | = f$的图像，为一个菱形），我们的目标就不仅是让原目标函数的值尽量小，还要让F越小越好。这样我们就需要找到一个合适的中间值。

我们知道当菱形的边与等高线相切时，菱形最小。根据公式

$$\tilde J(w;X,y) = J(w;X,y) + \alpha\Omega(w) $$

我们可以知道，当原函数取值不变的时候，$\Omega$越小，则目标函数越小。

实际上，对于很多原函数的等高线，与菱形相交的点很容易在坐标轴上，这也就是$L1$范数容易得到稀疏解的原因。

当使用L2正则化的时候，分析过程与L1正则化类似。从图像看，使用L2正则化的F不容易和等高线相交在坐标轴上，但会比较靠近坐标轴。所以L2范数能够让解比较小，但比较平滑。

#### 从贝叶斯先验概率的角度考虑

正则化是模型的参数估计增加了一个先验知识，会引导损失函数的最小值向约束方向迭代。L1正则化是拉普拉斯先验，L2正则化是高斯先验。

整个优化问题是一个最大后验估计，正则化项对应的是后验估计中的先验信息，损失函数对应的是后验估计中的似然函数，两者乘积就是贝叶斯最大后验估计。

$$\theta_{map} = arg \max_{\theta} (\prod_{i=1}^m p(y^(i) | x^{(i)};\theta))p(\theta) $$

在后面L1和L2的推导中，$armmax_{\mathbf w}L(\mathbf w) $即是将$\epsilon_i,\mathbf w_i $代入此公式。

一些数学基础知识：

###### 拉普拉斯分布

拉普拉斯分布的概率密度函数：

$$
\begin{align}
f(x|\mu,b) &= \frac{1}{2b}exp(-\frac{|x-\mu|}{b}) \\
&= \frac{1}{2b} \left\{\begin{matrix}
exp(-\frac{\mu-x}{b}), & if x < \mu \\ 
exp(-\frac{x-\mu}{b}), & if x \geqslant \mu
\end{matrix}\right.
\end{align}
$$

其中$\mu$是位置参数(数学期望)，$b>0$是尺度参数。如果$\mu = 0$，那么

$$f(x|0,b) = \frac{1}{2b}exp(-\frac{|x|}{b}) $$

恰好是尺度为$\frac{1}{2}$的指数分布。

![拉普拉斯分布概率密度](/img/Journal/拉普拉斯分布概率密度.png)

###### 高斯分布(正态分布)

高斯分布的概率密度函数为：

$$f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} $$

其中，$\mu$是数学期望，$\sigma^2$是标准方差。上式记为$X \sim N(\mu,\sigma^2)$。当$\mu = 0,\sigma^2 = 1 $时是标准正态分布。

![正态分布概率密度](/img/Journal/正态分布概率密度.png)

###### 推导

最小二乘公式的推导：

已知：

$$f(\mathbf x) = \sum_{j=1}^d x_iw_i + \epsilon  = \mathbf x \mathbf w^T + \epsilon, \mathbf x \in mathbb R^{1 \times d}, \mathbf w \in \mathbb R^{1\times d}, \epsilon \in \mathbb R.  $$

$$\mathbf X = (\mathbf x_1, ..., \mathbf x_n) \in \mathbb R^{n \times d}, \mathbf y \in \mathbb R^(n \times 1) $$

假设 $\epsilon \sim \mathcal N(0, \sigma^2) $,即 $\mathbf y_i \sim \mathcal N(\mathbf x_i \mathbf w^T, \sigma^2) $,用最大似然估计推导：

$$
\begin{align*}
argmax_{\mathbf w}L_{\mathbf w} &= ln\prod_{i=1}^n \frac{1}{\sigma \sqrt{2\pi}}exp(-\frac{1}{2}(\frac{mathbf y_i - \mathbf x_i\mathbf w^T}{sigma})^2) \\
&= -\frac{1}{2\sigma^2}\sum_{i=1}^n(\mathbf y_i-\mathbf x_i\mathbf w^T)^2 - nln\sigma\sqrt{2\pi} \\
argmin_{\mathbf w}f(\mathbf w) &= \sum_{i=1}^n(\mathbf y_i-\mathbf x_i\mathbf w^T)^2 \\
&= \| \mathbf y - \mathbf X\mathbf w^T \|_2^2
\end{align*}
$$

加入L2正则化后

&&
\begin{align*}
& \epsilon \sim \mathcal N(0, \sigma^2), \mathbf w_i \sim \mathcal N(0, \tau^2) \\
& arg min_{\mathbf w}f(\mathbf w) = \| \mathbf y - \mathbf X\mathbf w^T \|_2^2 + \lambda \| \mathbf w \|_2^2
\end{align*}
&&

![L2正则化推导](/img/Journal/L2正则化推导.png)

加入L1正则化后

&&
\begin{align*}
& \epsilon \sim \mathcal N(0, \sigma^2), \mathbf w_i \sim Laplace(0,b) \\
& arg min_{\mathbf w}f(\mathbf w) = \| \mathbf y - \mathbf X\mathbf w^T \|_2^2 + \lambda \| \mathbf w \|_1
\end{align*}
&&

![L1正则化推导](/img/Journal/L1正则化推导.png)
