---
layout:     post
title:      4.机器学习|决策树
subtitle:   机器学习周志华 学习笔记————机器学习|决策树
date:       2019-07-26
author:     正版慕言
header-img: img/blog_bg_3.jpg
catalog: true
tags:
    - 机器学习
    - 西瓜书

---

> 决策树是一种常见的机器学习方法，基于树结构来进行决策。

一般的，一棵决策树包含一个根节点、若干内部节点和若干叶节点。

决策树的生成是一个递归的过程，三种情况会导致递归返回：
1. 当前节点包含的样本全部属于同一类别无需划分
2. 当前属性集为空，或所有样本在所有属性上取值相同，无法划分（叶节点）
3. 当前节点包含的样本集合为空，不能划分（叶节点）

#### 划分选择
决策树学习的关键在于如何选择最优划分属性。
一般来说，随着划分的进行，我们希望决策树的分支节点包含的样本尽可能属于同一类别（纯度越来越高）。

☆**信息熵** 是度量样本集合纯度的最常用的一种指标。假设当前集合D中第k类样本所占的比例为pk，则信息熵定义如下：
![信息熵](/img/西瓜书-4.信息熵.gif)
**Ent的值越小，D的纯度越高。**

☆**信息增益** 对于一颗决策树来说，在它的所有节点中，样本数越多的分直接点其影响越大。我们可以计算出用有V个可能的取值的属性a对样本集D进行划分获得的“信息增益”：
![信息增益](/img/西瓜书-4.信息增益.gif)
一般来说，信息增益越大，意味着使用属性a进行划分获得的纯度提升越大。

**增益率** 有时候信息增益准则对可取值数目较多的属性有所偏好，而有时这种情况并不具有泛化能力。为了减少这种偏好的不利影响，可以不使用信息增益，而使用增益率来选择最优划分属性。
![增益率](/img/西瓜书-4.增益率.gif)
其中
![IV](/img/西瓜书-4.IV.gif)
称为属性a的“固有值”。a的取值数目越多，IV就越大。
增益率对可取值数目较少的属性有所偏好。

**基尼指数** 
数据集D的纯度可以用基尼指数来衡量：
![基尼指数](/img/西瓜书-4.基尼指数.gif)
它反映D中随机抽取两个样本其类别不一致的概率。因此，基尼指数越小，D的纯度越高。
基于上式，属性a的基尼指数可以这样定义：
![属性a的基尼指数](/img/西瓜书-4.属性a的基尼指数.gif)

#### 剪枝处理
剪枝是决策树对付过拟合的主要手段，是通过主动去掉一些分支来降低过拟合的风险。基本策略有**预剪枝**和**后剪枝**。评估使用第二章的性能评估方法。
预剪枝是在决策树生成过程中对每个节点在划分前先进行估计，如果不能带来泛化性能的提升，则停止划分并标记当前节点为叶节点。
后剪枝是先生成一棵完整的决策树，再自底向上对非叶节点进行考察，如果将当前子节点替换为叶节点能提升泛化性能，则将其替换为叶节点。

#### 连续与缺失值
**连续属性离散化技术**
二分法是最简单的离散化策略。以信息增益为依据
若当前节点的划分属性为连续属性，该属性还可以作为其后代节点的划分属性。

**缺失值的处理**
要解决的问题：
1. 如何在属性值缺失的情况下进行划分属性的选择
2. 给定划分属性，若样本在该属性上的值缺失，应该如何划分

取D在属性α上没有缺失的子集，计算信息增益进行决策。在划分时，若样本在α上的值已知，将其划入对应的子节点，并使样本权值在子节点中保持为ωx。否则，将样本同时划入所有子节点，样本权值在与属性av对应的子节点中调整为rv·ωx
信息增益的计算：
![属性值有缺失时的信息增益](/img/西瓜书-4.缺失值_信息增益.gif)

#### 多变量决策树
在普通的决策树中，分类边界都是与坐标轴平行的。虽然这样的分类边界具有良好的可解释性，但当分类边界比较复杂时，必须使用很多段划分才能获得较好的效果，这就导致决策树很复杂。
如果能使用斜的划分边界，就能够大大简化决策树模型。**多变量决策树** 可以实现这样的斜划分甚至更复杂的划分。
![单变量决策树的分类边界](/img/西瓜书-4.单变量决策树的分类边界.png)
![多变量决策树的分类边界](/img/西瓜书-4.多变量决策树的分类边界.png)
在实现斜划分的多变量决策树中，非叶节点不再是针对某一个属性，而是对属性的线性组合进行测试。每一个非叶节点是这样一个线性分类器：
![多变量决策树的线性分类器](/img/西瓜书-4.多变量决策树的线性分类器.gif)
与传统的单变量决策树不同，多变量决策树的学习过程中不是为每一个非叶节点寻找一个最优的划分属性，而是试图建立一个合适的线性分类器。