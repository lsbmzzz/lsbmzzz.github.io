---
layout:     post
title:      深度学习RNN的基本原理
subtitle:   深度学习RNN的基本原理
date:       2019-08-21
author:     正版慕言
header-img: img/blog_bg_1.jpg
catalog: true
mathjax: true
tags:
    - RNN
    - 深度学习

---

> 全连接神经网络和卷积神经网络每次都只能处理一个输入，也就是说两个不同的输入之间是完全没有关系的。但现在很多任务中我们需要能够处理一些序列信息，这些序列的前后输入之间是有关联的。例如我们处理视频的时候不能够单独处理其中的一帧，理解一句话的时候也不能只看其中的一个字或一个词。这时候，我们就需要**循环神经网络**（RNN）。

## 什么是循环神经网络RNN

#### RNN的结构
![6ee325d3906f10f2793fbb01e489930a.jpeg](/img/Journal/RNN/RNN示例.jpg)

上图是一个最简单的单隐层RNN。可以看到，RNN与CNN唯一的不同是其隐层产生的结果作为一个反馈重新送进了该层的输入。
以上图的RNN模型为例，用$x_t$表示该RNN在t时刻接收到的输入，$s_{t-1}$表示上一时刻隐藏层的值，$o_t$表示当前时刻的输入计算出的值，$U$表示输入层到隐层的权重矩阵，$V$表示隐层自身的权重矩阵，$W$表示隐层到输出层的权重矩阵，那么我们可以写出RNN的计算公式：

$$
\begin{align*}
o_t &= g(Vs_t) \\
&= f(Ux_t + Ws_{t-1})
\end{align*}
$$

可以看到，隐层的计算结果不仅与当前的输入x有关， 还与自己上一次计算出的s值有关。
如果将上式的s循环带入o则有

$$
\begin{align*}
o_t &= g(Vs_t) \\
&= Vf(Ux_t + Ws_{t-1}) \\
&= Vf(Ux_t + Wf(Ux_{t-1} + Ws_{t-2})) \\
&= Vf(Ux_t + Wf(Ux_{t-1} + Wf(Ux_{t-2} + ...)))
\end{align*}
$$

可以看到RNN的输出值o是受到历次输入的x的值的影响的。

#### 双向RNN
在一些序列任务中有时不仅要看前面的数据，还要看后面的数据。因此人们设计了双向RNN。
![0c8d438c12bce0260b16cf270db56f0b.png](/img/Journal/RNN/双向RNN.png)
双向RNN的隐藏层要保存两个值A和A'。其中，A参与正向计算，A'参与反向计算。输出值y可以表示为：

$$
\begin{align*} 
y_i &= g(VA_i + V'A'_i) \\
A_i &= f(WA_{i-1} + Ux_i) \\
A'_i &= f(W'A'_{i+1} + U'x_i)
\end{align*}
$$

据此写出双向RNN的计算方法：

$$
\begin{align*} 
o_t &= g(Vs_t + V's'_t) \\
s_t &= f(Ux_t + Ws_{t-1}) \\
s'_t &= f(U's_t + W's_{t+1})
\end{align*}
$$

**正向计算与反向计算不共享权重。**

#### 深度RNN
RNN可以扩展到多个隐层的形式，变成深度RNN。
![12384a06c2a2f14c23be0ffa0a706fdf.png](/img/Journal/RNN/深度RNN.png)
将计算公式推广到深度RNN，有：

$$
\begin{align*} 
o_t &= g(V^{(i)}s_t^{(i)} + V'^{(i)}s_t'^{(i)}) \\
s_t^{(i)} &= f(U^{(i)}s_t^{(i-1)} + W^{(i)}s_{t-1}) \\
s_t'^{(i)} &= f(U'^{(i)}s_t'^{(i-1)} + W'^{(i)}s_{t+1}') \\
...&\\
s_1^{(i)} &= f(U^{(1)}x_t + W^{(1)}s_{t-1}) \\
s_1'^{(i)} &= f(U'^{(1)}x_t + W'^{(1)}s_{t+1}') \\
\end{align*}
$$

## 