---
layout:     post
title:      深度学习RNN的基本原理
subtitle:   深度学习RNN的基本原理
date:       2019-08-21
author:     正版慕言
header-img: img/blog_bg_1.jpg
catalog: true
mathjax: true
tags:
    - RNN
    - 深度学习

---

> 全连接神经网络和卷积神经网络每次都只能处理一个输入，也就是说两个不同的输入之间是完全没有关系的。但现在很多任务中我们需要能够处理一些序列信息，这些序列的前后输入之间是有关联的。例如我们处理视频的时候不能够单独处理其中的一帧，理解一句话的时候也不能只看其中的一个字或一个词。这时候，我们就需要**循环神经网络**（RNN）。

## 什么是循环神经网络RNN

#### RNN的结构
![6ee325d3906f10f2793fbb01e489930a.jpeg](/img/Journal/NeuralNetworks/RNN示例.jpg)

上图是一个最简单的单隐层RNN。可以看到，RNN与CNN唯一的不同是其隐层产生的结果作为一个反馈重新送进了该层的输入。
以上图的RNN模型为例，用$x_t$表示该RNN在t时刻接收到的输入，$s_{t-1}$表示上一时刻隐藏层的值，$o_t$表示当前时刻的输入计算出的值，$U$表示输入层到隐层的权重矩阵，$V$表示隐层自身的权重矩阵，$W$表示隐层到输出层的权重矩阵，那么我们可以写出RNN的计算公式：

$$
\begin{align*}
o_t &= g(Vs_t) \\
&= f(Ux_t + Ws_{t-1})
\end{align*}
$$

可以看到，隐层的计算结果不仅与当前的输入x有关， 还与自己上一次计算出的s值有关。
如果将上式的s循环带入o则有

$$
\begin{align*}
o_t &= g(Vs_t) \\
&= Vf(Ux_t + Ws_{t-1}) \\
&= Vf(Ux_t + Wf(Ux_{t-1} + Ws_{t-2})) \\
&= Vf(Ux_t + Wf(Ux_{t-1} + Wf(Ux_{t-2} + ...)))
\end{align*}
$$

可以看到RNN的输出值o是受到历次输入的x的值的影响的。

#### 双向RNN
在一些序列任务中有时不仅要看前面的数据，还要看后面的数据。因此人们设计了双向RNN。
![0c8d438c12bce0260b16cf270db56f0b.png](/img/Journal/NeuralNetworks/双向RNN.png)
双向RNN的隐藏层要保存两个值A和A'。其中，A参与正向计算，A'参与反向计算。输出值y可以表示为：

$$
\begin{align*} 
y_i &= g(VA_i + V'A'_i) \\
A_i &= f(WA_{i-1} + Ux_i) \\
A'_i &= f(W'A'_{i+1} + U'x_i)
\end{align*}
$$

据此写出双向RNN的计算方法：

$$
\begin{align*} 
o_t &= g(Vs_t + V's'_t) \\
s_t &= f(Ux_t + Ws_{t-1}) \\
s'_t &= f(U's_t + W's_{t+1})
\end{align*}
$$

**正向计算与反向计算不共享权重。**

#### 深度RNN
RNN可以扩展到多个隐层的形式，变成深度RNN。
![12384a06c2a2f14c23be0ffa0a706fdf.png](/img/Journal/NeuralNetworks/深度RNN.png)
将计算公式推广到深度RNN，有：

$$
\begin{align*} 
o_t &= g(V^{(i)}s_t^{(i)} + V'^{(i)}s_t'^{(i)}) \\
s_t^{(i)} &= f(U^{(i)}s_t^{(i-1)} + W^{(i)}s_{t-1}) \\
s_t'^{(i)} &= f(U'^{(i)}s_t'^{(i-1)} + W'^{(i)}s_{t+1}') \\
...&\\
s_1^{(i)} &= f(U^{(1)}x_t + W^{(1)}s_{t-1}) \\
s_1'^{(i)} &= f(U'^{(1)}x_t + W'^{(1)}s_{t+1}') \\
\end{align*}
$$

## RNN的训练算法 BPTT

RNN的训练算法BPTT：针对循环层的训练算法，基本原理与BP算法相同。
算法步骤：
1. 前向计算每个神经元的输出值
2. 反向计算每个神经元的误差$\delta_j$
3. 计算每个权重的梯度

![RNN的循环层](/img/Journal/NeuralNetworks/RNN的循环层.png)

循环层的前向计算为$s_t = f(\mathbf{Ux_t + Ws_{t-1}}) $

其中，$x,s$都是向量，表示数据，$U,V,W$是矩阵，表示权重。

#### 误差的计算

BPTT算法第l层第t次输入的误差项$\delta_t^l$沿两个方向传播，一是向前传递到上一层网络，与权重矩阵$U$计算得到$\delta_t^{l-1}$，而是沿时间线向前传播，与矩阵$W$计算得到$\delta_{t-1}^l$。沿时间线的传播最终将到达初始t1时刻，得到$\delta_1^l$。

我们用$net_t$表示神经元在t时刻的输入

$$
\begin{align*}
net_t &= Ux_t + Ws_{t-1} \\
s_{t-1} = f(net_{t-1})
\end{align*}
$$

因此有

$$\frac{\partial net_t}{\partial net_{t-1}} = \frac{\partial net_t}{\partial s_{t-1}} \cdot \frac{\partial s_{t-1}}{\partial net_{t-1}} $$

我们将这两项分别展开计算，则第一项恰为矩阵W，第二项为f的对角矩阵。

$$
\begin{align*}
\frac{\partial net_t}{\partial s_{t-1}} &= W, \\
\frac{\partial s_{t-1}}{\partial net_{t-1}} &= diag[f'(net_{t-1})] \\
\frac{\partial net_t}{\partial net_{t-1}} &= Wdiag[f'(net_{t-1})] \\
&= \begin{bmatrix}
w_{11}f'(net_1^{t-1}) & w_{12}f'(net_2^{t-1}) & ... & w_{1n}f'(net_n^{t-1}) \\ 
w_{21}f'(net_1^{t-1}) & w_{22}f'(net_2^{t-1}) & ... & w_{2n}f'(net_n^{t-1}) \\ 
 & . &  &  \\ 
 & . &  &  \\ 
w_{n1}f'(net_1^{t-1}) & w_{n2}f'(net_2^{t-1}) & ... & w_{nn}f'(net_n^{t-1}) \\ 
\end{bmatrix}
\end{align*}
$$

###### 沿时间前向传播的误差项

上式描述了将$\delta$沿时间向前传播的规律。根据这一跪了我们可以求得任意时刻k的误差项$\delta_k$：

$$
\begin{align*}
\delta_k^T &= \frac{\partial E}{\partial net_k} \\
&= \frac{\partial E}{\partial net_t} \frac{\partial net_t}{\partial net_k} \\
&= \frac{\partial E}{\partial net_t} \frac{\partial net_t}{\partial net_{t-1}} ... \frac{\partial net_{k-1}}{\partial net_k} \\
&= Wdiag[f'(net_{t-1})]Wdiag[f'(net_{t-2})]...Wdiag[f'(net_k)]\delta_t^l \\
&= \delta_t^T \prod_{i=k}^{t-1}Wdiag[f'(net_i)]
\end{align*}
$$

这就是误差沿时间方向反向传播的公式。

误差传递到上一层网络的算法和BP是完全一样的。

#### 权重梯度的计算
![BPTT权重计算](/img/Journal/NeuralNetworks/BPTT权重计算.png)

首先要计算误差函数E对权重矩阵W的梯度$\frac{\partial E}{\partial W}$

我们有：$net_t = Ux_t + Ws_{t-1}$

对$W$求导，而对W求导与$Ux_i$无关，可以得到

$$
\begin{align*}
\frac{\partial E}{\partial w_{ji}} &= \frac{\partial E}{\partial net_j^t} \frac{\partial net_j^t}{\partial w_{ji}} \\
&= \delta_j^ts_i^{t-1}
\end{align*}
$$

在知道了任意时刻的误差项$\delta_t$和上一时刻循环层的输出$s_{t-1}$之后，我们可以求出权重矩阵在t时刻的梯度：

$$
\nabla_{W_t}E = \begin{bmatrix}
\delta_1^ts_1^{t-1} & \delta_1^ts_2^{t-1} & ... & \delta_1^ts_n^{t-1} \\
\delta_2^ts_1^{t-1} & \delta_2^ts_2^{t-1} & ... & \delta_2^ts_n^{t-1} \\
. & . & ... & . \\
. & . & ... & . \\
\delta_n^ts_1^{t-1} & \delta_n^ts_2^{t-1} & ... & \delta_n^ts_n^{t-1} \\
\end{bmatrix}
$$

最终的梯度$\nabla_WE$就是各个时刻的梯度之和：

$$\nabla_WE = \sum_{i=1}^t\nabla_{W_i}E$$

与权重矩阵W的算法类似，我们也可以得到权重矩阵U的计算方法

$$\nabla_UE = \sum_{i=1}^t\nabla_{U_i}E$$

## RNN的梯度爆炸和消失问题

在实际应用中，RNN往往不能很好地处理较长的序列，主要原因就是RNN在训练中很容易发生梯度爆炸和梯度消失，导致梯度不能在较长的序列中一直传递下去。

从上面误差传递的计算中我们可以看到，在传递过程中，每一步误差的计算都会用到前一步的计算结果。梯度的传播是一个指数函数，当传递的距离很远的时候，误差的增长或缩小就会非常快，从而导致梯度爆炸或梯度消失问题。

一般来说梯度爆炸比较容易处理，因为梯度爆炸会导致模型返回NaN的错误，我们可以对梯度设置一个上界来限制。而梯度消失比较难检测和处理。

应对梯度消失的几种办法：

1. 合理的初始化权值，每个神经元尽量不要取极大或极小值，避开梯度消失区域。
2. 使用好的激活函数（如ReLU）
3. 使用更好的RNN结构（如LSTM、GRU），这是流行的做法。