---
layout:     post
title:      16.机器学习|强化学习
subtitle:   机器学习周志华 学习笔记————机器学习|强化学习
date:       2019-08-05
author:     正版慕言
header-img: img/blog_bg_3.jpg
catalog: true
tags:
    - 机器学习
    - 西瓜书

---

#### 任务与奖赏
> 强化学习任务通常用马尔可夫决策过程来描述：机器处于环境E中，状态空间为X，器中每个状态x∈X是机器感知到的环境的描述，及其能采取的动作构成了动作空间A，若某个动作a∈A作用在当前状态x上，潜在的转移函数P将使环境从当前状态按某种概率转移到另一个状态，在转移到另一个状态时，环境会根据潜在的“奖赏”函数R反馈给机器一个奖赏。
> 综合起来，强化学习任务对应了四元组E=〈X,A,P,R〉，其中P：X×A×R→**R**指定了状态转移概率，R：X×A×X→**R**指定了奖赏。在所有应用中，奖赏函数可能仅与状态转移有关，即R：X×X→**R**。

![西瓜书-16.强化学习图示.png](/img/MachineLearning/西瓜书-16.强化学习图示.png)

强化学习中的“策略”实际上相当于监督学习中的分类器或回归器，模型的形式没有差别。但不同的是，强化学习中没有监督学习中的有标记样本。强化学习在某种意义上可以看作是具有“延迟标记信息”的监督学习问题。

#### K-摇臂赌博机

###### 探索与利用
一般的监督学习与强化学习有一个显著的不同：强化学习的最终奖赏是在多步动作之后才能观察到的。
考虑比较简单的情形：最大化单步奖赏。在这种情况下，机器也需要通过尝试来发现各种动作产生的结果，而没有训练数据告诉机器要做哪个动作。最大化单步奖赏要考虑两个方面：

* 需要知道每个动作带来的奖赏
* 要执行奖赏最大的动作

更一般的情形是：一个动作的奖赏是一个概率分布，仅通过一次尝试并不能确定的获得平均奖赏值。

实际上，单步强化学习任务对应了一个理论模型，即“K-摇臂赌博机”。

* 若果仅要知道每个摇臂的期望奖赏，则可以采用“仅探索”法：将所有尝试机会平均分配给所有摇臂，以每个摇臂各自的平均吐币率作为期望的近似估计；
* 如果仅为执行奖赏最大的动作，可以采用“仅利用”法：采用目前最优的摇臂，如果有相同，随机选择一个。

“探索”和“利用”是矛盾的，由于尝试次数有限，加强一个必然削弱另一个。

###### ε-贪心
ε-贪心算法基于一个概率来对探索和利用进行折中：每次以ε的概率进行探索，1 - ε的概率进行利用。
![西瓜书-16.ε-贪心算法.png](/img/MachineLearning/西瓜书-16.ε-贪心算法.png)

###### softmax
softmax算法基于当前已知的摇臂平均奖赏来对探索和利用进行折中。
softmax算法中的摇臂概率的分配基于Boltzmann分布：
![西瓜书-16.Boltzmann分布.gif](/img/MachineLearning/西瓜书-16.Boltzmann分布.gif)
Q(i)记录平均奖赏，τ为“温度”：τ越小则平均奖赏高的摇臂被选中的概率越高。
![西瓜书-16.softmax算法.png](/img/MachineLearning/西瓜书-16.softmax算法.png)

#### 有模型学习
如果四元组四元组E=〈X,A,P,R〉已知，这样的情形称为“模型已知”。在已知模型的环境中学习称为“有模型学习”。

###### 策略评估
在模型已知时，对任意策略π能估计出该策略带来的期望累计奖赏。
基于T步累积奖赏的策略评估算法：
![西瓜书-16.基于T步累积奖赏的策略评估算法.png](/img/MachineLearning/西瓜书-16.基于T步累积奖赏的策略评估算法.png)

###### 策略改进
对某个策略的累积奖赏进行评估后，若发现它并非最优策略，就希望对它进行改进。理想的策略应该最大化累积奖赏：
![西瓜书-16.最大化累积奖赏.gif](/img/MachineLearning/西瓜书-16.最大化累积奖赏.gif)
一个强化学习任务可能有多个最优策略，最优策略对应的函数值V* 称为最优值函数：
![西瓜书-16.最优值函数.gif](/img/MachineLearning/西瓜书-16.最优值函数.gif)
由于最优值函数的累积奖赏值已经达到最大，因此利用最优Bellman等式
![西瓜书-16.最优Bellman等式.gif](/img/MachineLearning/西瓜书-16.最优Bellman等式.gif)
得到唯一最优解就是最优值函数。

对于当前策略π，可以放心的将其改进为：
![西瓜书-16.改进π.gif](/img/MachineLearning/西瓜书-16.改进π.gif)
直到π与π'一致就满足了最优Bellman等式，即找到了最优策略。

###### 策略迭代与值迭代
将前面两节结合起来就得到了求最优解的方法：
> 从一个处事策略出发，先进行策略评估，然后改进策略，评估改进的策略，再进一步改进... 不断迭代进行评估和改进，直到策略收敛不再改变。这样的做法叫做“策略迭代”。

![西瓜书-16.基于T步累积奖赏的策略迭代算法.png](/img/MachineLearning/西瓜书-16.基于T步累积奖赏的策略迭代算法.png)

![西瓜书-16.基于T步累积奖赏的值迭代算法.png](/img/MachineLearning/西瓜书-16.基于T步累积奖赏的值迭代算法.png)

如果采用γ折扣累积奖赏，只要把上图算法中的第3行改为
![西瓜书-16.γ值迭代.gif](/img/MachineLearning/西瓜书-16.γ值迭代.gif)

#### 免模型学习
> 在现实的强化学习任务中，环境的转移概率、奖赏函数往往难以得知，甚至很难知道环境中有多少状态。如果学习算法不依赖于环境建模，就称为“免模型学习”。这比有模型学习困难得多。

###### 蒙特卡罗强化学习
免模型情况下，会遇到以下问题：

* 策略无法评估：一种直接的替代方法是多次采样，去平均累积奖赏作为期望的近似。
* 策略迭代算法估计的是状态值函数V，当模型未知时，也会出现困难：可以将估计对象从V变成Q。
* 模型未知时，机器只能从一个起始状态开始探索环境

同策略蒙特卡罗强化学习算法
![西瓜书-16.同策略蒙特卡罗强化学习算法.png](/img/MachineLearning/西瓜书-16.同策略蒙特卡罗强化学习算法.png)

同策略蒙特卡罗强化学习算法最终产生的是ε-贪心策略。

异策略蒙特卡罗强化学习算法
![西瓜书-16.异策略蒙特卡罗强化学习算法.png](/img/MachineLearning/西瓜书-16.异策略蒙特卡罗强化学习算法.png)

###### 时序差分学习
蒙特卡罗强化学习算法通过考虑采样轨迹，客服了模型未知给策略估计造成的困难。但由于没有充分利用强化学习任务的MDP结构，效率很低。
时序差分学习（TD）结合了动态规划与蒙特卡罗方法的思想，能够做到更高效的免模型学习。

Sarsa算法是一个同策略算法
![西瓜书-16.Sarsa算法.png](/img/MachineLearning/西瓜书-16.Sarsa算法.png)
Sarsa算法中评估、执行的均为ε-贪心策略。

将Sarsa修改为异策略算法，就得到了Q-学习算法
![西瓜书-16.Q-学习算法.png](/img/MachineLearning/西瓜书-16.Q-学习算法.png)
Q-学习算法评估的是原始策略，执行的是ε-贪心策略。

#### 值函数近似
前面我们都是假设强化学习任务是在有限状态空间上进行的，而现实强化学习任务面临的状态空间往往是连续的，有无穷多个状态。
一个想法是对状态空间进行离散化，但在对状态空间进行探索之前，如何离散化是个难题。

我们可以直接对连续状态空间的值函数进行学习。假定状态空间为n维的实数空间X = **R**^n，先考虑值函数能表达为状态的线性函数的简单情形：
![西瓜书-16.值函数简单情形.gif](/img/MachineLearning/西瓜书-16.值函数简单情形.gif)
采用梯度下降法，用最小二乘误差度量近似程度，可以得到对于单个样本的更新规则
![西瓜书-16.单个样本的更新规则.gif](/img/MachineLearning/西瓜书-16.单个样本的更新规则.gif)
我们不知道策略的真实值函数V^π，但可借助时序差分学习，用当前估计的值函数替代真实的值函数
![西瓜书-16.估计值函数.gif](/img/MachineLearning/西瓜书-16.估计值函数.gif)
其中x'表示下一时刻的状态。

基于线性值函数近似替代Sarsa算法中的值函数，就能得到线性值函数近似Sarsa算法。
![西瓜书-16.线性值函数近似Sarsa算法.png](/img/MachineLearning/西瓜书-16.线性值函数近似Sarsa算法.png)

#### 模仿学习
在强化学习中，及其获得的反馈信息仅有多步决策后的累积奖赏，但现实任务中往往能得到人类专家的决策过程范例。从范例中学习称为“模仿学习”。

###### 直接模仿学习
强化学习中，多步决策的搜索空间巨大，基于累积奖赏学习非常困难。直接模仿人类专家的“状态-动作对”可以显著缓解这一困难，我们称为“直接模仿学习”。

我们可以将“状态-动作对”抽取出来，构造一个新的数据集D，把状态作为特征，动作作为标记，对这个新数据集使用分类或回归算法得到策略模型。这一模型可以作为强化学习的初始策略，通过强化学习基于环境反馈进行改进，从而获得更好的策略。

###### 逆强化学习
在很多任务中，设计奖赏函数往往比较困难，从人类专家提供的范例数据中反推出奖赏函数有助于解决该问题。这就是“逆强化学习”。

逆强化学习的基本思想：欲使机器做出与范例一致的行为，等价于在某个奖赏函数的环境中求解最优策略，该最优策略所产生的轨迹与范例数据一致。

我们难以得到所有策略，一个比较好的办法是从随机策略开始迭代的求解更好的奖赏函数，基于奖赏函数获得更好的策略。
![西瓜书-16.迭代式逆强化学习算法.png](/img/MachineLearning/西瓜书-16.迭代式逆强化学习算法.png)
