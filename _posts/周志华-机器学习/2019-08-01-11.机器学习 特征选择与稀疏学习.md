---
layout:     post
title:      11.机器学习|特征选择与稀疏学习
subtitle:   机器学习周志华 学习笔记————机器学习|特征选择与稀疏学习
date:       2019-08-01
author:     正版慕言
header-img: img/blog_bg_3.jpg
catalog: true
tags:
    - 机器学习
    - 西瓜书

---

#### 子集搜索与评价
对一个学习任务来说，有的属性可能很关键，有的属性可能没什么用。有用的属性称为“相关特征”，无用的属性称为“无关特征”。选出相关特征子集的过程称为“特征选择”。
特征选择是一个重要的数据预处理过程。进行特征选择的原因：

* 减轻维数灾难
* 降低学习任务的难度

特征选择必须确保不丢失重要特征，否则会影响获得的性能。
“冗余特征”包含的信息能从其他特征推演出来，他们在很多时候不起作用，但有时候可以降低学习任务的难度。

特征选择可行的做法是产生一个“候选子集”，评价其好坏，基于评价结果产生下一个候选子集，以此循环至无法找到更好的候选子集为止。
关键：

* 如何根据评价结果获取下一个候选特征子集？
* 如何评价候选特征子集的好坏？

1. 子集搜索问题
    1. “前向”搜索：对一个特征集合，第一次选择一个最优特征，然后每次在上一次的基础上增加一个新的剩余的最优特征。直到第k+1个特征时特征子集不如上一轮，保留前k个并停止。
    2. “后向”搜索：从完整集合开始，每次删除一个。
    3. “双向”搜索：每轮增加特征（增加的不会被删除），同时减少无关特征。

2. 子集评价问题
给定数据集D，对属性子集A，根据其取值将D分成V个子集，每个子集中的样本在A上取值相同，则A的信息增益和信息熵：
![西瓜书-11.特征选择与稀疏学习-信息增益与信息熵.gif](/img/MachineLearning/西瓜书-11.特征选择与稀疏学习-信息增益与信息熵.gif)
Gain(A)越大，A包含的有助于分类的信息越多。我们可以对每个候选特征子集基于D计算信息增益作为评价准则。
将搜索机制和评价机制结合，就能得到特征选择方法。

常见的特征选择方法有三种：**过滤式、包裹式**和**嵌入式**。

#### 过滤式选择
过滤式方法先对数据集进行特征选择，然后训练学习器，特征选择过程与后续学习器无关。相当于先用特征选择过程对初始特征进行“过滤”，再用过滤后的特征来训练模型。

典型的过滤式特征选择方法：**Relief**
该方法设计了一个称为“相关统计量”的向量来度量特征的重要性。其每个分量分别对应一个初始特征，特征子集的重要性由子集中每个特征对应的相关统计量分量的和来确定。可以最终指定一个τ选择比τ大的分量对应的特征，或者指定一个k选择前k个大的分量对应的特征。
**Relief的关键在于如何确定相关统计量。**给定一个训练集，对每个示例**x**i，现在同类样本中找到最邻近的x1称为“猜中临近”，再从其异类样本寻找最邻近x2称为“猜错临近”。如果到x1的距离更近，增大对应的统计量；反之，减小相对应的统计量。
![西瓜书-11.特征选择与稀疏学习-Relief统计量更新.gif](/img/MachineLearning/西瓜书-11.特征选择与稀疏学习-Relief统计量更新.gif)
Relief是针对二分类设计的，扩展变体Relief-F能处理多分类问题：
![西瓜书-11.特征选择与稀疏学习-Relief-F统计量更新.gif](/img/MachineLearning/西瓜书-11.特征选择与稀疏学习-Relief-F统计量更新.gif)

#### 包裹式选择
包裹式选择直接把最终将要使用的学习器的性能作为特征子集的评价准则。
包裹式选择由于直接针对给定的学习器进行优化，因此从最终性能看应该比过滤式特征选择更好。但因为在选择中要多次训练学习器，因此计算开销比较大。
典型的包裹式特征选择方法：**LVW**(Las Vegas Wrapper)
![西瓜书-11.LVW算法.png](/img/MachineLearning/西瓜书-11.LVW算法.png)

#### 嵌入式选择与L1正则化
嵌入式特征选择将特征选择与学习器训练融为一体，在同一个优化过程中完成。
对给定数据集D，考虑最简单的线性回归模型，以平方误差为损失函数，可以获得优化目标：
![西瓜书-11.特征选择与稀疏学习-嵌入式优化目标.gif](/img/MachineLearning/西瓜书-11.特征选择与稀疏学习-嵌入式优化目标.gif)
样本特征多而数据少时，用L1范数正则化：
![西瓜书-11.特征选择与稀疏学习-嵌入式优化目标L1正则化.gif](/img/MachineLearning/西瓜书-11.特征选择与稀疏学习-嵌入式优化目标L1正则化.gif)

> sklearn提供了SelectFromModel实现嵌入式特征提取。

#### 稀疏表示与字典学习
把数据集D考虑成一个矩阵，每行对应一个样本，每列对应一个特征，形成一个矩阵。
为普通的稠密表达的样本找到合适的字典，将样本转化为合适的稀疏表示形式，从而使学习任务得到简化，模型复杂度得以降低，通常称为“字典学习”：
![西瓜书-11.特征选择与稀疏学习-字典学习.gif](/img/MachineLearning/西瓜书-11.特征选择与稀疏学习-字典学习.gif)
其中xi为第i个样本，B为字典矩阵，αi为xi的稀疏表示，λ为大于0参数。
采用变量交替优化的策略来求解。分两步：

* 固定字典B，参照LASSO解法求解，为每个样本xi找到αi：
![西瓜书-11.特征选择与稀疏学习-为每个x找到α.gif](/img/MachineLearning/西瓜书-11.特征选择与稀疏学习-为每个x找到α.gif)

* 以αi为初值更新字典B：
![西瓜书-11.特征选择与稀疏学习-更新字典B.gif](/img/MachineLearning/西瓜书-11.特征选择与稀疏学习-更新字典B.gif)

反复迭代以上两步，最终可求得字典B和样本xi的稀疏表示αi。用户可以设置k的大小来控制字典规模。

#### 压缩感知
现实任务中我们常希望能根据部分信息恢复出全部信息。压缩感知为解决这类问题提供了新思路。
与特征选择、稀疏表示不同，压缩感知关注的是如何利用信号本身具有的稀疏性，从部分观测样本中恢复原信号。通常认为，压缩感知分为“**感知测量**”和“**重构恢复**”两个阶段。

* 感知测量关注如何对原始信号进行处理以获得稀疏样本表示，涉及到傅里叶变换、小波变换以及字典学习、稀疏编码等。
* 重构恢复关注如何基于稀疏性从少量观测中恢复原信号，是压缩感知的精髓。我们谈到压缩感知时，通常指的是这部分。

能通过压缩感知技术恢复欠采样信号的前提条件之一是信号有稀疏表示。**矩阵补全技术**能够用于解决补全欠采样信号的问题。