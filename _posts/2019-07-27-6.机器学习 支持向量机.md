---
layout:     post
title:      6.机器学习|支持向量机
subtitle:   机器学习周志华 学习笔记————机器学习|支持向量机
date:       2019-07-27
author:     正版慕言
header-img: img/blog_bg_3.jpg
catalog: true
tags:
    - 机器学习
    - 西瓜书
    - 支持向量机

---

#### 间隔与支持向量

> 最大间隔、对偶问题、KKT条件、核函数、最优下界 等。

分类学习的基本思想是基于训练集D在样本空间中找到一个划分超平面，把不同类别的样本分开。
![西瓜书-6.存在多个超平面.png](/img/西瓜书-6.存在多个超平面.png)
划分超平面的描述：
![西瓜书-6.划分超平面.gif](/img/西瓜书-6.划分超平面.gif)
因此，样本空间中任意点到划分超平面的距离可以表示为：
![西瓜书-6.到超平面的距离.gif](/img/西瓜书-6.到超平面的距离.gif)
如果超平面能将样本正确分类，那么有：
![西瓜书-6.超平面.gif](/img/西瓜书-6.超平面.gif)
距离超平面最近的训练样本点使等号成立，他们被称为支持向量。两个异类支持向量到超平面的距离之和被称为间隔。
![西瓜书-6.支持向量与间隔.png](/img/西瓜书-6.支持向量与间隔.png)
根据间隔公式，要寻找具有最大间隔的划分超平面，只要最小化||ω||^2。因此有
![西瓜书-6.最大化间隔.gif](/img/西瓜书-6.最大化间隔.gif)
这就是支持向量机（SVM）的基本型。

#### 对偶问题
求解最大间隔划分超平面所对应的模型：
![西瓜书-6.最大间隔划分超平面的模型.gif](/img/西瓜书-6.最大间隔划分超平面的模型.gif)

对SVM的基本型每项增加拉格朗日乘子，可得到其拉格朗日函数。
![西瓜书-6.拉格朗日函数.gif](/img/西瓜书-6.拉格朗日函数.gif)
令L对ω和b的偏导数为0，得到**对偶问题**：
![西瓜书-6.对偶问题.gif](/img/西瓜书-6.对偶问题.gif)

解出α，去除ω和b就得到模型：
![西瓜书-6.对偶模型.gif](/img/西瓜书-6.对偶模型.gif)
上面过程满足KKT（Karush-Kuhn-Tucher）条件:
![西瓜书-6.KKT条件.gif](/img/西瓜书-6.KKT条件.gif)

**支持向量机的一个重要性质**：训练完成后，大部分的训练样本不需要保留，最终模型只与支持向量有关。

**序列最小优化算法SMO**求解对偶问题：
基本思路：先固定α_i之外的所有参数，α_i上的极值。由于有s.t.约束，α_i可由其他变量导出。SMO选择α_i和α_j两个变量，并固定其它参数，这样在初始化后SMO不断执行下面个两个步骤直到收敛：
* 选取一对α_i和α_j；
* 固定它们之外的参数，求解对偶问题获得更新后的α_i和α_j。
选取的α_i和α_j间隔尽量大，对他们进行更新时可以带给目标函数值更多大的变化。

> SMO算法为什么高效？
> 在固定其它参数后，仅优化两个参数的过程能做到非常高效。

#### 核函数
现实中的问题：原始的样本空间内也许并不存在一个能正确划分两类样本的超平面。
解决方法：将原始样本空间映射到一个更高维的特征空间。
> 如果原始空间是有限维的，那么一定存在一个高维特征空间使样本可分。

如果用Φ(**x**)表示**x**映射后的特征向量，在特征空间中划分超平面所对应的模型可以表示为：![西瓜书-6.高维映射_特征模型.gif](/img/西瓜书-6.高维映射_特征模型.gif)
其中ω和b是模型参数。
它的对偶问题是：
![西瓜书-6.高维映射_对偶问题.gif](/img/西瓜书-6.高维映射_对偶问题.gif)
设想一个如下的函数：
![西瓜书-6.高维映射_设想函数.gif](/img/西瓜书-6.高维映射_设想函数.gif)
如此一来，上面的对偶问题就可以写为：
![西瓜书-6.高维映射_对偶问题重写.gif](/img/西瓜书-6.高维映射_对偶问题重写.gif)
求解得到：
![西瓜书-6.高维映射_对偶问题求解.gif](/img/西瓜书-6.高维映射_对偶问题求解.gif)
这里的κ(·,·)就是**核函数**。上式的最优解可以通过训练样本的核函数展开。这一展式也称为“支持向量展式”。

---
在现实任务中，我们通常不知道φ(·)是什么形式，我们有如下定理：
***令χ为输入空间，κ(·,·)是定义在χ×χ上的对称函数，则κ是核函数当且仅当对任意数据D = {x1, x2, ..., xm}，“核矩阵”K总是半正定的。***
![西瓜书-6.核矩阵.png](/img/西瓜书-6.核矩阵.png)

在不知道特征映射的形式时，我们不知道什么样的核函数才是合适的。核函数的选择是支持向量机的最大变数，如果核函数选择不合适，意味着将样本映射到了一个不合适的特征空间，很可能造成性能不佳。
![西瓜书-6.常用核函数.png](/img/西瓜书-6.常用核函数.png)
另外还可以核函数组合得到：
1. 如果κ1和κ2都是核函数，那么对于任意整数γ1和γ2，有 **γ1κ1 + γ2κ2** 也是核函数。
2. 如果κ1和κ2都是核函数，那么核函数的直积 κ1×κ2(**x**, **z**) = κ1(**x**, **z**)κ2(**x**, **z**) 也是核函数。
3. 若κ1为核函数，则对于任意函数g(**x**)，κ(**x**, **z**) = g(**x**)κ1(**x**, **z**)g(**z**) 也是核函数。

#### 软间隔与正则化

在现实任务中，往往很难确定合适的核函数。即使恰好找到了某个看似合适的核函数，也不能确定线性可分的结果是不是过拟合造成的。
缓解的方法是允许SVM在一些样本上出错。因此引入了“**软间隔**”的概念。
![西瓜书-6.软间隔.png](/img/西瓜书-6.软间隔.png)
在最大化间隔的同时，不满足约束的样本应尽可能少。优化目标可以写为：
![西瓜书-6.高维映射_软间隔优化目标.gif](/img/西瓜书-6.高维映射_软间隔优化目标.gif)
其中0/1损失函数：
![西瓜书-6.01损失函数.gif](/img/西瓜书-6.01损失函数.gif)
但它的数学性质不够好，因此通常使用“替代损失”函数。三种常用的替代损失函数：
![西瓜书-6.替代损失函数.gif](/img/西瓜书-6.替代损失函数.gif)
![西瓜书-6.三种替代损失函数.png](/img/西瓜书-6.三种替代损失函数.png)

**采用hinge损失的情况：**
上面的优化目标变为：
![西瓜书-6.hinge优化目标.gif](/img/西瓜书-6.hinge优化目标.gif)
引入松弛变量ξ_i >= 0， 上式可写为：
![西瓜书-6.hinge优化目标_引入松弛变量.gif](/img/西瓜书-6.hinge优化目标_引入松弛变量.gif)
这就是常用**“软间隔支持向量机”**。

通过拉格朗日乘子法可以得到上式的拉格朗日函数：
![西瓜书-6.hinge优化目标_引入松弛变量的拉格朗日函数.gif](/img/西瓜书-6.hinge优化目标_引入松弛变量的拉格朗日函数.gif)
其中α_i >= 0, μ_i >= 0。
令L对**ω**，b，ξ_i的偏导数分别为0得到
![西瓜书-6.hinge优化目标_引入松弛变量的拉格朗日函数_求偏导.gif](/img/西瓜书-6.hinge优化目标_引入松弛变量的拉格朗日函数_求偏导.gif)
带入上面的拉格朗日函数，得到其对偶问题：
![西瓜书-6.hinge优化目标_对偶问题.gif](/img/西瓜书-6.hinge优化目标_对偶问题.gif)
可以看出，其与应间隔唯一的区别就是对偶变量的约束不同。应间隔约束是0 <= α_i，它是0 <= α_i <= C。
对软间隔支持向量机，其KKT条件要求：
![v](/img/西瓜书-6.hinge优化目标_KKT.gif)
软间隔支持向量机的最终模型仅与支持向量有关，通过采用hinge损失函数后仍然保持了洗稀疏性。

---

如果使用对率损失函数，就得到了对率回归模型。对率回归模型的优势在于其输出具有自然的概率意义。且对率回归模型可以直接应用于多分类任务，而SVM需要进行推广才可以。
hinge损失有一块零区域，使支持向量机的解具有稀疏性。对率损失是光滑的单调递减函数，因此对率损失依赖更多的训练样本，预测开销更大。

---

我们还可以把损失函数替换成其他，得到不同的学习模型。它们的性质与使用的替代函数相关，但具有以下共性：
* 优化目标中的第一项用来描述划分超平面的间隔大小
* 另一项用来表述训练集上的误差。
可以写为一般形式：
![西瓜书-6.优化目标的一般形式.gif](/img/西瓜书-6.优化目标的一般形式.gif)
式中，前者为“结构风险”，描述模型f的某些性质；后者为“经验风险”，描述模型与训练数据的契合程度。

该式也可称为正则化问题，式中前者称为正则化项，C称为正则化常数。

#### 支持向量回归SVR
回归问题：传统的回归模型通过f(x)与y之间的差别来计算损失，当二者完全重合时，损失才为0。支持向量回归容忍f(x)与y之间最多有ε的偏差，超过这个偏差才计算损失。
![西瓜书-6.支持向量回归示意图.png](/img/西瓜书-6.支持向量回归示意图.png)

因此，SVR问题可以写为：
![西瓜书-6.SVR问题.gif](/img/西瓜书-6.SVR问题.gif)

C是正则化常数，ℓ_ε是图中所示的 不敏感损失 函数。
![西瓜书-6.不敏感损失函数.gif](/img/西瓜书-6.不敏感损失函数.gif)
![西瓜书-6.不敏感损失函数.png](/img/西瓜书-6.不敏感损失函数.png)

引入松弛变量和拉格朗日乘子，得到SVR问题的拉格朗日函数：
![西瓜书-6.SVR的拉格朗日函数.gif](/img/西瓜书-6.SVR的拉格朗日函数.gif)

我们有![西瓜书-6.最大间隔划分超平面的模型.gif](/img/西瓜书-6.最大间隔划分超平面的模型.gif)，带入然后令ω, b, ξ和ξ^的偏导都为0，然后代入回去得到SVR的对偶问题：
![西瓜书-6.SVR对偶问题.gif](/img/西瓜书-6.SVR对偶问题.gif)

KKT条件：
![西瓜书-6.SVR-KKT.gif](/img/西瓜书-6.SVR-KKT.gif)

SVR的解形式：
![西瓜书-6.SVR解形式.gif](/img/西瓜书-6.SVR解形式.gif)
考虑特征映射，SVR可以表示为：
![西瓜书-6.SVR特征映射解形式.gif](/img/西瓜书-6.SVR特征映射解形式.gif)
κ为核函数。

#### 核方法
> ***表示定理*** ：令**H**为核函数κ对应的再生希尔伯特空间，||h||_ **H**表示**H**空间中关于h的范数，对于任意单调递增函数Ω：
> [0, ∞]→**R**和任意非负损失函数ℓ：**R**^m→[0, ∞]，优化问题
> ![西瓜书-6.核方法表示定理.gif](/img/西瓜书-6.核方法表示定理.gif)
> 的解总可以写为：
> ![西瓜书-6.核方法表示定理的解形式.gif](/img/西瓜书-6.核方法表示定理的解形式.gif)

表示定理对于损失函数没有限制，对Ω只要求单调递增。这意味着对于一般的损失函数和正则化项，上述优化问题的最优解都可以表示为核函数κ的线性组合。

人们发展出一系列基于核函数的学习方法统称为“**核方法**”。
最常见的是通过“核化”（引入核函数）来将线性学习器拓展为非线性学习器，得到“核线性判别分析”（Kernelized Linear Discriminant Analysis, KLDA）。

先假设通过某种映射将样本映射到一个特征空间**F**，在F中执行线性判别分析求得
![西瓜书-6.KLDA执行线性判别分析.gif](/img/西瓜书-6.KLDA执行线性判别分析.gif)
KLDA的学习目标：
![西瓜书-6.KLDA学习目标.gif](/img/西瓜书-6.KLDA学习目标.gif)

我们难以知道映射φ的具体形式，因此用核函数κ隐式的表达特征空间F。用J(ω)做损失函数，Ω = 0，得到函数h(**x**)，然后经过优化函数的解得到ω。
![西瓜书-6.KLDA_h函数与ω.gif](/img/西瓜书-6.KLDA_h函数与ω.gif)
令：
![西瓜书-6.KLDA_设置条件.gif](/img/西瓜书-6.KLDA_设置条件.gif)
则KLDA的学习目标等价为：
![西瓜书-6.KLDA等价学习目标.gif](/img/西瓜书-6.KLDA等价学习目标.gif)
