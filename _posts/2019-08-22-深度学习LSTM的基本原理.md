---
layout:     post
title:      深度学习LSTM的基本原理
subtitle:   深度学习LSTM的基本原理
date:       2019-08-22
author:     正版慕言
header-img: img/blog_bg_1.jpg
catalog: true
mathjax: true
tags:
    - LSTM
    - 深度学习

---

> 长短时记忆网络(Long Short Term Memory Networks, LSTM)是为了解决RNN难以处理长距离依赖的缺陷而产生的一种改进的循环神经网络。

## RNN的梯度消失

在循环神经网络中，误差沿时间反向传播的公式：

$$
\begin{align*}
\delta_k^T &= \delta_t^T\prod_{i=k}^{t-1}diag[f'(net_i)]W \\
\|\delta_k^T\| & \leqslant \|\delta_t^T\|\prod_{i=k}^{t-1}\|diag[f'(net_i)]\|\|W\| \\
&\leqslant \|\delta_t^T\|(\beta_f\beta_W)^{t-k}
\end{align*}
$$

可以看到误差传播过程中上界$\beta_f\beta_W$是一个指数函数，所以当$t-k$很大时很容易发生梯度消失和梯度爆炸。

在RNN中，权重W最终的梯度$\nabla_WE=\Sigma_{k=1}^t\nabla_{W_k}E$，可以据此画出各时刻的梯度关系：

![](/img/Journal/NeuralNetworks/RNN的梯度消失.png)

很明显在t-3开始梯度就几乎为0了，之后的梯度几乎不会对W产生影响。

## 长短时记忆网络LSTM

RNN的隐藏层只有一个输入状态h，从上一节可以知道它对短期输入很敏感。LSTM增加了一个状态c，用来保存一个长期状态。新增的状态c被称为**单元状态**。

![从RNN到LSTM](/img/Journal/NeuralNetworks/RNN到LSTM.png)

将上图按照时间维度展开：

![LSTM展开](/img/Journal/NeuralNetworks/LSTM展开.png)

从展开图中可以看到在任意时刻t，LSTM的输入有三个：

1. 当前的输入值$x_t$
2. LSTM上一时刻的输出值$h_{t-1}$
3. LSTM上一时刻的单元状态$c_{t-1}$

LSTM的输出值有两个：

1. LSTM当前时刻的输出值$h_t$
2. LSTM当前时刻的单元状态$c_t$

**LSTM的关键在于如何控制长期状态c**。LSTM使用了三个控制开关来控制c：

1. 第一个开关负责控制和保持长期状态c
2. 第二个开关负责控制即时状态到长期状态c的输入
3. 第三个开关负责控制是否将长期状态c加入LSTM的输出h

## LSTM的前向计算

LSTM使用“门”(gate)来控制开关。门实际上是一个全连接层。

$$g(x) = \sigma(\mathbf {Wx+b})$$

其中$W$是门的权重矩阵，$\mathbf b$是偏置，$\sigma$是一个sigmoid函数。

LSTM用**遗忘门**控制$\mathbf {c_{t-1}}$有多少保留到$\mathbf {c_t}$，用**输入门**控制当前时刻的$\mathbf {x_t}$有多少保存到$\mathbf {c_t}$，用**输出门**控制$\mathbf {c_t}$有多少输出到了LSTM的输出值$\mathbf {h_t}$。

#### 遗忘门(forget gate)

$$\mathbf {f_t} = \sigma (W_f \cdot [\mathbf {h_{t-1},x_t}] + \mathbf {b_f}) $$

其中，$W_f$表示遗忘门的权重矩阵，$[\mathbf {h_{t-1}, x_t}]$是两个向量的连接，$\mathbf {b_f}$是遗忘门的偏置项，$\sigma$是sigmoid函数。

如果输入维度是$d_x$，隐藏层维度是$d_h$，单元状态的维度是$d_c$（一般的，$d_c = d_h$）则$W_f$的维度是$d_c \times (d_h + d_x)$。权重矩阵$W_f$实际上是由$W_{fh}$和$W_{fx}$两个矩阵拼接得到的，前者对应输出项$h_{t-1}$，后者对应输入项$x_t$。

遗忘门的计算图解：

![遗忘门的计算](/img/Journal/NeuralNetworks/遗忘门的计算.png)

#### 输入门(input gate)

$$\mathbf {i_t} = \sigma(W_i \cdot [\mathbf {h_{t-1}, x_t}] + \mathbf {b_i}) $$

输入门的计算图解：

![输入门的计算](/img/Journal/NeuralNetworks/输入门的计算.png)

#### $\mathbf {c_t}$的计算

输出门的输出$c_t$要考虑上一次的单元状态$\mathbf {c_{t-1}}$和当前输入的单元状态$\mathbf {\widetilde c_t}$两部分。

$\mathbf {\widetilde c_t}$的计算：

$$\mathbf {\widetilde c_t} = tanh (W_c \cdot [\mathbf {h_{t-1}, x_t}] + \mathbf {b_c}) $$

$\mathbf {\widetilde c_t}$的计算图解：

![当前状态c的计算](/img/Journal/NeuralNetworks/当前c的计算.png)

此时便可以计算$\mathbf {c_t}$:

$$\mathbf {c_t} = f_t \circ \mathbf {c_{t-1}} + i_t \circ \mathbf {\widetilde {c_t}} $$

$\mathbf {c_t}$的计算图解：

![c的计算](/img/Journal/NeuralNetworks/c的计算.png)

LSTM通过上面的计算将$\mathbf {\widetilde c_t}$和$\mathbf {c_{t-1}}$结合在一起得到$\mathbf {c_t}$。

#### 输出门(output gate)

$$\mathbf {o_t} = \sigma(W_o \cdot [\mathbf {h_{t-1}, x_t}] + \mathbf {b_o}) $$

输出门的计算图解：

![输出门的计算](/img/Journal/NeuralNetworks/输出门的计算.png)

#### LSTM的最终输出

最终输出由输出门和单元状态共同决定：

$$\mathbf {h_t} = \mathbf {o_t} \circ tanh(\mathbf {c_t}) $$

最终输出的计算图解：

![最终输出的计算](/img/Journal/NeuralNetworks/h的计算.png)


## LSTM的训练

训练过程：

1. 前向计算每个神经元的输出值。在LSTM中，有$\mathbf f_t, \mathb i_t, \mathbf c_t, \mathbf o_t, \mathbf h_t$五个向量值。
2. 反向计算每个神经元的误差$\delta$。LSTM的误差向时间和网络两个方向传播。
3. 根据相应的误差计算每个权重的梯度。

#### 基础

1. 门的激活使用sigmoid函数，输出的激活使用tanh函数。他们的导数：

$$
\begin{align*}
\sigma (z) &= y = \frac{1}{1 + e^{-z}} \\
\sigma' (z) &= y(1-y) \\
tanh(z) = y = \frac{e^z - e^{-z}}{e^z + e^{-z}} \\
tanh'(z) = 1-y^2
\end{align*}
$$

2. LSTM要学习的参数：遗忘门、输入门、输出门、单元状态分别的权重矩阵和偏置项$W_f, \mathbf b_f, W_i, \mathbf b_i, W_o, \mathbf b_o, W_c, \mathbf b_c$。其中W是两个矩阵的拼接，可以拆分表示为$W_{fh}, W_{fx}, W_{ih}, W_{ix}, W_{oh}, W_{ox}, W_{ch}, W_{cx}$。

3. $\circ$的运算：

* 两个向量按乘：

$$
\mathbf{a}\circ\mathbf{b}=\begin{bmatrix}
a_1\\a_2\\a_3\\...\\a_n
\end{bmatrix}\circ\begin{bmatrix}
b_1\\b_2\\b_3\\...\\b_n
\end{bmatrix}=\begin{bmatrix}
a_1b_1\\a_2b_2\\a_3b_3\\...\\a_nb_n
\end{bmatrix}
$$

* 向量和矩阵按乘：

$$
\begin{align}
\mathbf{a}\circ X&=\begin{bmatrix}
a_1\\a_2\\a_3\\...\\a_n
\end{bmatrix}\circ\begin{bmatrix}
x_{11} & x_{12} & x_{13} & ... & x_{1n}\\
x_{21} & x_{22} & x_{23} & ... & x_{2n}\\
x_{31} & x_{32} & x_{33} & ... & x_{3n}\\
& & ...\\
x_{n1} & x_{n2} & x_{n3} & ... & x_{nn}\\
\end{bmatrix}\\
&=\begin{bmatrix}
a_1x_{11} & a_1x_{12} & a_1x_{13} & ... & a_1x_{1n}\\
a_2x_{21} & a_2x_{22} & a_2x_{23} & ... & a_2x_{2n}\\
a_3x_{31} & a_3x_{32} & a_3x_{33} & ... & a_3x_{3n}\\
& & ...\\
a_nx_{n1} & a_nx_{n2} & a_nx_{n3} & ... & a_nx_{nn}\\
\end{bmatrix}
\end{align}
$$

* 两个矩阵按乘：

$$
\begin{align*}
X \circ Y &= \begin{bmatrix}
x_{11} & x_{12} & x_{13} & ... & x_{1n}\\
x_{21} & x_{22} & x_{23} & ... & x_{2n}\\
x_{31} & x_{32} & x_{33} & ... & x_{3n}\\
... & ... & ... & ... &...\\
x_{n1} & x_{n2} & x_{n3} & ... & x_{nn}\\
\end{bmatrix} \circ \begin{bmatrix}
y_{11} & y_{12} & y_{13} & ... & y_{1n}\\
y_{21} & y_{22} & y_{23} & ... & y_{2n}\\
y_{31} & y_{32} & y_{33} & ... & y_{3n}\\
... & ... & ... & ... &...\\
y_{n1} & y_{n2} & y_{n3} & ... & y_{nn}\\
\end{bmatrix} \\ 
&= \begin{bmatrix}
x_{11}y_{11} & x_{12}y_{12} & x_{13}y_{13} & ... & x_{1n}y_{1n}\\
x_{21}y_{21} & x_{22}y_{22} & x_{23}y_{23} & ... & x_{2n}y_{2n}\\
x_{31}y_{31} & x_{32}y_{32} & x_{33}y_{33} & ... & x_{3n}y_{3n}\\
... & ... & ... & ... &...\\
x_{n1}y_{n1} & x_{n2}y_{n2} & x_{n3}y_{n3} & ... & x_{nn}y_{nn}\\
\end{bmatrix}
\end{align*}
$$

* 对角矩阵右乘一个矩阵：

$$
diag[\mathbf a]X = \mathbf a \circ X
$$

* 一个行向量右乘一个对角矩阵：

$$
\mathbf a^T diag[b] = a \circ b
$$

#### LSTM的反向传播计算

在LSTM中，有$\mathbf f_t, \mathbf i_t, \mathbf o_t, \mathbf c_t$四个加权输入，而我们希望往上一层传递的误差是一个。因此我们假设误差项$\delta_t$是损失函数对输出值的导数而不是对加权输入$net_t^l$的导数。在t时刻LSTM的输出为$\mathbf h_t$，我们定义

$$\delta_t = \overset {def}{=} \frac{\partial E}{\partial \mathbf h_t} $$

仍然定义四个加权输入和它们对应的误差项：

$$
\begin{align*}
\mathbf{net}_{f,t} &= W_f[h_{t-1},x_t] + b_f \\
&= W_{fh}h_{t-1} + W_{fx}x_t + b_f \\
\delta_{f,t} \overset{def}{&=}\frac{\partial E}{\partial \mathbf{net}_{f,t}} \\
\mathbf{net}_{i,t} &= W_i[h_{t-1},x_t] + b_i \\
&= W_{ih}h_{t-1} + W_{ix}x_t + b_i \\
\delta_{i,t} \overset{def}{&=}\frac{\partial E}{\partial \mathbf{net}_{i,t}} \\
\mathbf{net}_{\tilde c,t} &= W_c[h_{t-1},x_t] + b_c \\
&= W_{fc}h_{t-1} + W_{cx}x_t + b_c \\
\delta_{\tilde c,t} \overset{def}{&=}\frac{\partial E}{\partial \mathbf{net}_{\tilde c,t}} \\
\mathbf{net}_{o,t} &= W_o[h_{t-1},x_t] + b_o \\
&= W_{oh}h_{t-1} + W_{ox}x_t + b_o \\
\delta_{o,t} \overset{def}{&=}\frac{\partial E}{\partial \mathbf{net}_{o,t}} \\
\end{align*}
$$

#### 误差沿时间的反向传递

误差沿时间反向传递，要就算出在t时刻上一时刻的误差项$\delta_{t-1}$

$$
\begin{align}
\delta_{t-1}^T &= \frac{\partial E}{\partial \mathbf h_{t-1}} \\
&= \frac{\partial E}{\partial \mathbf h_t} \frac{\partial \mathbf h_t}{\partial \mathbf h_{t-1}} \\
&= \delta_t^T \frac{\partial \mathbf h_t}{\partial \mathbf h_{t-1}}
\end{align}
$$

其中，$\frac{\partial \mathbf h_t}{\partial \mathbf h_{t-1}}$是一个雅可比矩阵。假设隐藏层h的维度是N，则它是一个NXN的矩阵。可以哄$\mathbf h_t$的计算公式求它：

$$
\begin{align*}
\mathbf h_t &= \mathbf o_t \circ tanh(\mathbf c_t) \\
\mathbf c_t = \mathbf f_t \circ \mathbf c_{t-1} +\mathbf i_t \circ \tilde {\mathbf c}_t
\end{align*}
$$

利用全导数公式，有

$$
\begin{align}
\delta_t^T\frac{\partial{\mathbf{h_t}}}{\partial{\mathbf{h_{t-1}}}}&=\delta_t^T\frac{\partial{\mathbf{h_t}}}{\partial{\mathbf{o}_t}}\frac{\partial{\mathbf{o}_t}}{\partial{\mathbf{net}_{o,t}}}\frac{\partial{\mathbf{net}_{o,t}}}{\partial{\mathbf{h_{t-1}}}}
+\delta_t^T\frac{\partial{\mathbf{h_t}}}{\partial{\mathbf{c}_t}}\frac{\partial{\mathbf{c}_t}}{\partial{\mathbf{f_{t}}}}\frac{\partial{\mathbf{f}_t}}{\partial{\mathbf{net}_{f,t}}}\frac{\partial{\mathbf{net}_{f,t}}}{\partial{\mathbf{h_{t-1}}}}
+\delta_t^T\frac{\partial{\mathbf{h_t}}}{\partial{\mathbf{c}_t}}\frac{\partial{\mathbf{c}_t}}{\partial{\mathbf{i_{t}}}}\frac{\partial{\mathbf{i}_t}}{\partial{\mathbf{net}_{i,t}}}\frac{\partial{\mathbf{net}_{i,t}}}{\partial{\mathbf{h_{t-1}}}}
+\delta_t^T\frac{\partial{\mathbf{h_t}}}{\partial{\mathbf{c}_t}}\frac{\partial{\mathbf{c}_t}}{\partial{\mathbf{\tilde{c}}_{t}}}\frac{\partial{\mathbf{\tilde{c}}_t}}{\partial{\mathbf{net}_{\tilde{c},t}}}\frac{\partial{\mathbf{net}_{\tilde{c},t}}}{\partial{\mathbf{h_{t-1}}}}\\
&=\delta_{o,t}^T\frac{\partial{\mathbf{net}_{o,t}}}{\partial{\mathbf{h_{t-1}}}}
+\delta_{f,t}^T\frac{\partial{\mathbf{net}_{f,t}}}{\partial{\mathbf{h_{t-1}}}}
+\delta_{i,t}^T\frac{\partial{\mathbf{net}_{i,t}}}{\partial{\mathbf{h_{t-1}}}}
+\delta_{\tilde{c},t}^T\frac{\partial{\mathbf{net}_{\tilde{c},t}}}{\partial{\mathbf{h_{t-1}}}}
\end{align}
$$

将上式中的所有偏导数都求出来

根据

$$
\begin{align*}
\mathbf h_t &= \mathbf o_t \circ tanh(\mathbf c_t) \\
\mathbf c_t = \mathbf f_t \circ \mathbf c_{t-1} +\mathbf i_t \circ \tilde {\mathbf c}_t
\end{align*}
$$

分别能够求出

$$
\begin{align*}
\frac{\partial \mathbf h_t}{\partial \mathbf o_t} &= diag[tanh(\mathbf c_t] \\
\frac{\partial \mathbf h_t}{\partial \mathbf c_t} &= diga[\mathbf o_t \circ (1 - tanh(\mathbf c_t)^2)] \\
\\
\frac{\partial \mathbf c_t}{\partial \mathbf f_t} &= diag[\mathbf c_{t-1}] \\
\frac{\partial \mathbf c_t}{\partial \mathbf i_t} &= diag[\tilde {\mathbf c})t] \\
\frac{\partial \mathbf c_t}{\partial \tilde{\mathbf c}_t} &= diag[\mathbf i_t] \\
\end{align*}
$$

有

$$
\begin{align*}
\mathbf o_t &= \sigma(\mathbf{net}_{o,t}) \\
\mathbf{net}_{o,t} &= W_{oh}\mathbf h_{t-1} + W_{ox}\mathbf x_t + \mathbf b_o \\
\mathbf f_t &= \sigma(\mathbf{net}_{f,t}) \\
\mathbf{net}_{f,t} &= W_{fh}\mathbf h_{t-1} + W_{fx}\mathbf x_t + \mathbf b_o \\
\mathbf i_t &= \sigma(\mathbf{net}_{i,t}) \\
\mathbf{net}_{i,t} &= W_{ih}\mathbf h_{t-1} + W_{ix}\mathbf x_t + \mathbf b_o \\
\mathbf {c}_t &= tanh (\mathbf {net}_{c,t}) \\
\mathbf{net}_{\tilde c,t} &= W_{ch}\mathbf h_{t-1} + W_{cx}\mathbf x_t + \mathbf b_o \\
\end{align*}
$$

可以求得

$$
\begin{align}
\frac{\partial \mathbf o_t}{\partial \mathbf {net}_{o,t}} &= diag[\mathbf o_t \circ (1 - \mathbf o_t)] \\
\frac{\partial \mathbf {net}_{o,t}}{\partial \mathbf h_{t-1}} &= W_{oh} \\
\frac{\partial \mathbf f_t}{\partial \mathbf {net}_{f,t}} &= diag[\mathbf f_t \circ (1 - \mathbf f_t)] \\
\frac{\partial \mathbf {net}_{f,t}}{\partial \mathbf h_{t-1}} &= W_{fh} \\
\frac{\partial \mathbf i_t}{\partial \mathbf {net}_{i,t}} &= diag[\mathbf i_t \circ (1 - \mathbf i_t)] \\
\frac{\partial \mathbf {net}_{i,t}}{\partial \mathbf h_{t-1}} &= W_{ih} \\
\frac{\partial \mathbf {\tilde c}_t}{\partial \mathbf {net}_{{\tilde c},t}} &= diag[1-\mathbf{\tilde c}_t^2] \\
\frac{\partial \mathbf {net}_{{\tilde c},t}}{\partial \mathbf h_{t-1}} &= W_{ch}
\end{align}
$$

带入到$\delta_{t-1}$可以得到误差沿时间反向传播的一个时刻的公式：

$$
\begin{align}
\delta_{t-1}&=\delta_{o,t}^T\frac{\partial{\mathbf{net}_{o,t}}}{\partial{\mathbf{h_{t-1}}}}
+\delta_{f,t}^T\frac{\partial{\mathbf{net}_{f,t}}}{\partial{\mathbf{h_{t-1}}}}
+\delta_{i,t}^T\frac{\partial{\mathbf{net}_{i,t}}}{\partial{\mathbf{h_{t-1}}}}
+\delta_{\tilde{c},t}^T\frac{\partial{\mathbf{net}_{\tilde{c},t}}}{\partial{\mathbf{h_{t-1}}}} \\ 
&= \delta_{o,t}^TW_{oh} + \delta_{f,t}^TW_{fh} + \delta_{i,t}^TW_{ih} + \delta_{\telda c,t}^TW_{ch}, \\
\delta_{o,t}^T &= \delta_t^T \circ tanh(\mathbf c_t) \circ \mathbf o_t \circ (1-\mathbf o_t), \\
\delta_{f,t}^T &= \delta_t^T \circ \mathbf o_t \circ (1 - tanh(\mathbf c_t)^2) \circ \mathbf c_{t-1} \circ \mathbf f_t \circ (1 - \mathbf f_t), \\
\delta_{i,t}^T &= \delta_t^T \circ \mathbf o_t \circ (1 - tanh(\mathbf c_t)^2) \circ \mathbf {\tilde c}_t \circ \mathbf i_t \circ (1 - \mathbf i_t), \\
\delta_{\tilde c, t}^T &= \delta_t^T \circ \mathbf o_t \circ (1 - tanh(\mathbf c_t)^2) \circ \mathbf i_t \circ (1 - \mathbf {\tilde c}^2)
\end{align}
$$

据此可以写出将误差向前传播到任意时刻k的公式：

$$\delta_k^T = \prod_{j=k}^{t-1} \delta_{o,j}^TW_{oh} + \delta_{f,j}^TW_{fh} + \delta_{i,j}^TW_{ih} + \delta_{\tilde c, j}^TW_{ch} $$

#### 误差向上一层的传递
