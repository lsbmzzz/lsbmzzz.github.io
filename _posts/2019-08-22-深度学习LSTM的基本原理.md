---
layout:     post
title:      深度学习LSTM的基本原理
subtitle:   深度学习LSTM的基本原理
date:       2019-08-22
author:     正版慕言
header-img: img/blog_bg_1.jpg
catalog: true
mathjax: true
tags:
    - LSTM
    - 深度学习

---

> 长短时记忆网络(Long Short Term Memory Networks, LSTM)是为了解决RNN难以处理长距离依赖的缺陷而产生的一种改进的循环神经网络。

## RNN的梯度消失

在循环神经网络中，误差沿时间反向传播的公式：

$$
\begin{align*}
\delta_k^T &= \delta_t^T\prod_{i=k}^{t-1}diag[f'(net_i)]W \\
\|\delta_k^T\| & \leqslant \|\delta_t^T\|\prod_{i=k}^{t-1}\|diag[f'(net_i)]\|\|W\| \\
&\leqslant \|\delta_t^T\|(\beta_f\beta_W)^{t-k}
\end{align*}
$$

可以看到误差传播过程中上界$\beta_f\beta_W$是一个指数函数，所以当$t-k$很大时很容易发生梯度消失和梯度爆炸。

在RNN中，权重W最终的梯度$\nabla_WE=\Sigma_{k=1}^t\nabla_{W_k}E$，可以据此画出各时刻的梯度关系：

![](/img/Journal/NeuralNetworks/RNN的梯度消失.png)

很明显在t-3开始梯度就几乎为0了，之后的梯度几乎不会对W产生影响。

## 长短时记忆网络LSTM

RNN的隐藏层只有一个输入状态h，从上一节可以知道它对短期输入很敏感。LSTM增加了一个状态c，用来保存一个长期状态。新增的状态c被称为**单元状态**。

![从RNN到LSTM](/img/Journal/NeuralNetworks/RNN到LSTM.png)

将上图按照时间维度展开：

![LSTM展开](/img/Journal/NeuralNetworks/LSTM展开.png)

从展开图中可以看到在任意时刻t，LSTM的输入有三个：

1. 当前的输入值$x_t$
2. LSTM上一时刻的输出值$h_{t-1}$
3. LSTM上一时刻的单元状态$c_{t-1}$

LSTM的输出值有两个：

1. LSTM当前时刻的输出值$h_t$
2. LSTM当前时刻的单元状态$c_t$

**LSTM的关键在于如何控制长期状态c**。LSTM使用了三个控制开关来控制c：

1. 第一个开关负责控制和保持长期状态c
2. 第二个开关负责控制即时状态到长期状态c的输入
3. 第三个开关负责控制是否将长期状态c加入LSTM的输出h

## LSTM的前向计算

LSTM使用“门”(gate)来控制开关。门实际上是一个全连接层。

$$g(x) = \sigma(\mathbf {Wx+b})$$

其中$W$是门的权重矩阵，$\mathbf b$是偏置，$\sigma$是一个sigmoid函数。

LSTM用**遗忘门**控制$\mathbf {c_{t-1}}$有多少保留到$\mathbf {c_t}$，用**输入门**控制当前时刻的$\mathbf {x_t}$有多少保存到$\mathbf {c_t}$，用**输出门**控制$\mathbf {c_t}$有多少输出到了LSTM的输出值$\mathbf {h_t}$。

#### 遗忘门(forget gate)

$$\mathbf {f_t} = \sigma (W_f \cdot [\mathbf {h_{t-1},x_t}] + \mathbf {b_f}) $$

其中，$W_f$表示遗忘门的权重矩阵，$[\mathbf {h_{t-1}, x_t}]$是两个向量的连接，$\mathbf {b_f}$是遗忘门的偏置项，$\sigma$是sigmoid函数。

如果输入维度是$d_x$，隐藏层维度是$d_h$，单元状态的维度是$d_c$（一般的，$d_c = d_h$）则$W_f$的维度是$d_c \times (d_h + d_x)$。权重矩阵$W_f$实际上是由$W_{fh}$和$W_{fx}$两个矩阵拼接得到的，前者对应输出项$h_{t-1}$，后者对应输入项$x_t$。

遗忘门的计算图解：

![遗忘门的计算](/img/Journal/NeuralNetworks/遗忘门的计算.png)

#### 输入门(input gate)

$$\mathbf {i_t} = \sigma(W_i \cdot [\mathbf {h_{t-1}, x_t}] + \mathbf {b_i}) $$

输入门的计算图解：

![输入门的计算](/img/Journal/NeuralNetworks/输入门的计算.png)

#### $\mathbf {c_t}$的计算

输出门的输出$c_t$要考虑上一次的单元状态$\mathbf {c_{t-1}}$和当前输入的单元状态$\mathbf {\widetilde c_t}$两部分。

$\mathbf {\widetilde c_t}$的计算：

$$\mathbf {\widetilde c_t} = tanh (W_c \cdot [\mathbf {h_{t-1}, x_t}] + \mathbf {b_c}) $$

$\mathbf {\widetilde c_t}$的计算图解：

![当前状态c的计算](/img/Journal/NeuralNetworks/当前c的计算.png)

此时便可以计算$\mathbf {c_t}$:

$$\mathbf {c_t} = f_t \circ \mathbf {c_{t-1}} + i_t \circ \mathbf {\widetilde {c_t}} $$

$\mathbf {c_t}$的计算图解：

![c的计算](/img/Journal/NeuralNetworks/c的计算.png)

LSTM通过上面的计算将$\mathbf {\widetilde c_t}$和$\mathbf {c_{t-1}}$结合在一起得到$\mathbf {c_t}$。

#### 输出门(output gate)

$$\mathbf {o_t} = \sigma(W_o \cdot [\mathbf {h_{t-1}, x_t}] + \mathbf {b_o}) $$

输出门的计算图解：

![输出门的计算](/img/Journal/NeuralNetworks/输出门的计算.png)

#### LSTM的最终输出

最终输出由输出门和单元状态共同决定：

$$\mathbf {h_t} = \mathbf {o_t} \circ tanh(\mathbf {c_t}) $$

最终输出的计算图解：

![最终输出的计算](/img/Journal/NeuralNetworks/h的计算.png)


## LSTM的训练