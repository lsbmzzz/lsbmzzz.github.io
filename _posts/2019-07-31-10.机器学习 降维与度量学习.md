---
layout:     post
title:      10.机器学习|降维与度量学习
subtitle:   机器学习周志华 学习笔记————机器学习|降维与度量学习
date:       2019-07-31
author:     正版慕言
header-img: img/blog_bg_3.jpg
catalog: true
tags:
    - 机器学习
    - 西瓜书
    - 降维与度量学习

---

#### k近邻学习
k近邻学习是一种常用的监督学习方法。工作机制：给定测试样本，基于某种距离度量找出训练集中与其最靠近的k个训练样本，然后基于这k个“邻居”的信息来进行预测。
在分类任务中，可以使用投票法，以出现最多的标记作为预测结果；在回归任务中，可以使用平均法，以平均值作为预测结果。还可以基于距离远近进行加权。

k近邻学习有一个明显的不同在于它没有显式的训练过程。这被称为“ **懒惰学习** ”。相应的，在训练阶段就对样本进行学习处理的方式被称为“ **急切学习** ”。
k近邻分类器的示意图：
![西瓜书-10.k近邻分类器示意图.png](/img/西瓜书-10.k近邻分类器示意图.png)

* k的取值会对分类结果造成显著的影响
* 不同的距离计算方式也会使找出的“近邻”有显著差别

发现：虽然最近邻分类器简单，但它的泛化错误率不超过贝叶斯最优分类器的错误率的两倍。

#### 低维嵌入
上一节的讨论中有一个重要的假设：样本的采样密度足够大，在x任意小的邻域内总能找到一个训练样本。在现实任务中，当属性纬度比较高时，这样的条件是很难满足的，甚至计算内积也是困难的。

在高维情况下遇到的样本稀疏和距离计算困难等问题是所有机器学习方法共同面对的障碍，被称为“维数灾难”。

缓解“维数灾难”的一个重要途径是降维，通过某种数学变换将原始的高维属性空间映射到一个低维的“子空间”上。
![西瓜书-10.低维嵌入示意图.png](/img/西瓜书-10.低维嵌入示意图.png)

经典的降维方法：多维缩放（MDS）
假设m个样本在原始空间的距离矩阵**D**∈**R** ^ (m * m), dist_ij为xi到xj的距离，我们的目标是获得样本在d'维空间上的表示**Z**∈**R** ^ (d' * m)，d' ≤ d，且任意两个样本在d'空间中的欧氏距离等于其在原始空间中的距离，可以求得内积矩阵**B**：
![西瓜书-10.求降维后的内积矩阵B.gif](/img/西瓜书-10.求降维后的内积矩阵B.gif)

对矩阵**B** 做特征值分解**B = VΛV^T** （Λ为特征值构成的对角矩阵），λ降序，令V* 表示相应的特征向量矩阵，有
![西瓜书-10.MDS降维空间Z.gif](/img/西瓜书-10.MDS降维空间Z.gif)
MDS算法流程：
![西瓜书-10.MDS算法.png](/img/西瓜书-10.MDS算法.png)

一般来说，降维最简单的是对原始空间进行线性变换：**Z = W^T X**，其中**W**=**R**^(d x d')。

#### 主成分分析（Principal Component Analysis, PCA）
PCA是一种常用的降维方法。
> 考虑：对于正交属性空间中的样本点，如何用一个超平面对所有样本进行恰当表达？
> 如果存在这样的超平面，那么它应该有这样的性质：
> 
> 1. **最近重构性**：样本点到这个超平面的距离都足够近
> 2. **最大可分性**：样本点在这个超平面上的投影尽可能分开

基于以上两点性质，我们能分别得到主成分分析的两种等价推导。

1. 从最近重构性出发
假设数据样本进行过中心化，投影后的d维新坐标系**W**为**标准正交基向量集**。要将维度降低到d' < d，样本点**z**i = (zi1, zi2, ..., zid')，其中zij = **ω**j^T **ω**是**x**i在低维坐标系下的第j维的坐标。
基于z来重构x，会得到：
![西瓜书-10.基于z重构x.gif](/img/西瓜书-10.基于z重构x.gif)

考虑整个训练集。原样本点与基于投影重构的样本点之间的距离为：
![西瓜书-10.原样本点与重构样本点的距离.gif](/img/西瓜书-10.原样本点与重构样本点的距离.gif)
根据最近重构性，应该将上式最小化，所以主成分分析的优化目标为：
![西瓜书-10.最近重构性优化目标.gif](/img/西瓜书-10.最近重构性优化目标.gif)

2. 从最大可分性出发
从最大可分性出发，能够得到主成分分析的另一种解释：样本点**x**i在新空间中超平面上的投影为**W**^T **x**i，要让所有样本点投影尽可能的分开，那么投影后的样本方差应该最大化。因此：
![西瓜书-10.最大可分性优化目标.gif](/img/西瓜书-10.最大可分性优化目标.gif)

PCA算法流程：
![西瓜书-10.PCA算法.png](/img/西瓜书-10.PCA算法.png)

降维后的d'通常都是用户指定的，或者通过在d'值不同的低维空间中对k近邻分类器进行交叉验证来确定d'的值。对于PCA，还可以从重构的角度设置一个重构阈值（例如t = 95%），然后选取使下式成立的最小d'值：
![西瓜书-10.PCA阈值.gif](/img/西瓜书-10.PCA阈值.gif)

将样本映射到低维空间中后，低维空间与原始高维空间必有不同，因为有d-d'的特征被舍弃了。但这往往是必要的，因为：

* 舍弃这部分信息后能够使样本的采样密度增大，这是降维的重要动机
* 当数据收到噪声影响时，最小的特征值对应的特征向量往往与噪声有关，舍弃他们有一定的去噪效果。

#### 核化线性降维
在不少现实任务中，可能需要非线性映射才能找到恰当的低维嵌入。如下图所示，样本从二维空间中的采样后以S形嵌入三维空间，如果用线性降维将失去其原本的低维结构。我们将原本的低维空间称为“本真”低维空间。
![西瓜书-10.线性降维失去原本的低维结构.png](/img/西瓜书-10.线性降维失去原本的低维结构.png)

非线性降维的一种常用方法是基于核技巧对线性降维方法进行“核化”。
例如：**核主成分分析：**
假设我们要将高维空间中的数据土偶应到d维的**W**确定的超平面上，对于**w**j，有：
![西瓜书-10.降维的wj.gif](/img/西瓜书-10.降维的wj.gif)

假设**z**i由**x**i经过映射φ产生，φ能被显式的表达出来，则：通过它将样本映射到高维特征空间，再在特征空间中实施PCA即可。
![西瓜书-10.wj_φ.gif](/img/西瓜书-10.wj_φ.gif)
通过φ到高维空间的映射：
![西瓜书-10.φ到高维特征空间的映射.gif](/img/西瓜书-10.φ到高维特征空间的映射.gif)
而通常我们不知道φ的形式，所以引入核函数，将上式化简
![西瓜书-10.引入核函数.gif](/img/西瓜书-10.引入核函数.gif)
对新样本**x**，其投影后的j维坐标为：
![西瓜书-10.x投影后的第j维坐标.gif](/img/西瓜书-10.x投影后的第j维坐标.gif)
可以看出，味蕾获得投影后的坐标，KPCA需要对所有样本求和，因此计算开销较大。

#### 流形学习
流形学习是一类借鉴了拓扑流形概念的降维方法。“流形”是在局部与欧式空间同胚的空间（在局部具有欧式空间的性质，可以用欧氏距离来进行距离计算）。
这给降维方法带来了启发：若低维流形嵌入高维空间中，则数据样本在高维空间的分布看似复杂，但在局部仍然具有欧式空间的性质。因此可以很容易的在局部建立降维映射关系，然后设法将局部映射关系推广到全局。当维度被降为二到三维时，可以进行可视化展示，因此流形学习也能够被用于可视化。
两种著名的流形学习方法：等度量映射和局部线性嵌入。

###### 等度量映射（Isomap）
等度量映射认为低维度流形嵌入高维空间后，直接在高维空间中计算直线距离具有误导性，因为高位空间中的直线距离在低维嵌入流形上不可达。低维嵌入流形上两点之间的距离应是“测地线”距离（红线）。
此时我们可以利用流形在局部上与欧式空间同胚的性质，对每个点基于欧氏距离找到其近邻点，建立一个近邻连接图。
![西瓜书-10.近邻距离.png](/img/西瓜书-10.近邻距离.png)
如图，计算两点之间测地线距离的问题，被转化为计算近邻连接图上两点之间的最短路径问题。如此能获得测地线距离很好的逼近。
![西瓜书-10.Isomap算法.png](/img/西瓜书-10.Isomap算法.png)

###### 局部线性嵌入（LLE）
局部线性嵌入试图保持邻域内样本之间的线性关系。

先为每个样本**x**i找到其近邻下标集合Qi，然后计算出基于Qi的样本点对**x**i进行线性重构的系数**w**i：
![西瓜书-10.LLE-x重构为w.gif](/img/西瓜书-10.LLE-x重构为w.gif)
LLE在低维空间中**w**i不变，那么xi对应的低维空间坐标zi可以这样求解：
![西瓜书-10.LLE-求Z.gif](/img/西瓜书-10.LLE-求Z.gif)
![西瓜书-10.LLE算法.png](/img/西瓜书-10.LLE算法.png)

其中，式10.27为上面的wij，式10.30为上面的**M**。

#### 度量学习
在机器学习中，对高维数据进行降维的主要目的是希望找到一个合适的低维空间，再此空间中进行学习能比原始空间性能更好。实际上，每个空间对应了样本属性上定义的一个距离度量，寻找合适的空间，实际上就是在寻找一个合适的距离度量。
度量学习的基本动机就是**直接“学习”出一个距离度量。**

要对距离度量进行学习，必须有一个便于学习的距离度量表达形式。固定的参数不能通过对数据样本的学习加以改善，因此，需要引入一个可调节的参数。
对于两个d维的样本xi和xj，将他们之间的欧氏距离写为：
![西瓜书-10.度量学习-欧氏距离.gif](/img/西瓜书-10.度量学习-欧氏距离.gif)
假设不同属性的重要性不同，引入权重**w**，有
![西瓜书-10.度量学习-欧氏距离引入权重.gif](/img/西瓜书-10.度量学习-欧氏距离引入权重.gif)
其中**W**是对角矩阵。上式中**W**可以通过学习来确定。
进一步的：**W**对非对角元素全部为0，表示坐标轴是正交的。但现实任务中往往不是这样，对相关的属性，他们不是正交的。因此，将**W**替换为一个普通的半正定对称矩阵**M**，得到马氏距离：
![西瓜书-10.度量学习-马氏距离.gif](/img/西瓜书-10.度量学习-马氏距离.gif)
其中**M**称为“度量矩阵”，度量学习就是对其进行学习。**M**必须是（半）正定对称矩阵，必须有正交基**P**使**M = PP**^T。

对**M**的学习要设置一个目标。以近邻成分分析为例：（NCA）
近邻分类器判别时通常使用多数投票法，邻域中的每个样本投1票，邻域外的样本投0票。将其替换为概率投票法，则有：
![西瓜书-10.度量学习-概率投票.gif](/img/西瓜书-10.度量学习-概率投票.gif)
当i = j时，p最大。显然xj对xi的影响随着距离增大而减小。以留一法正确率的最大化为目标，xi被自身之外的所有样本分类正确的概率为：
![西瓜书-10.度量学习-留一分类正确率.gif](/img/西瓜书-10.度量学习-留一分类正确率.gif)
整个样本集上的留一法正确率为：
![西瓜书-10.度量学习-样本集的留一分类正确率.gif](/img/西瓜书-10.度量学习-样本集的留一分类正确率.gif)
将pij带入，考虑**M=PP**^T，则NCA的优化目标为：
![西瓜书-10.度量学习-NCA优化目标.gif](/img/西瓜书-10.度量学习-NCA优化目标.gif)
求解上式，可得最大化近邻分类器LOO正确率的距离度量矩阵M。

我们不仅能把错误率这样的监督学习目标作为度量学习的优化目标，也能引入邻域知识。若已知某些样本相似、某些样本不相似，则可以定义“必连”约束集合M和“勿连”约束集合C，(xi, xj)∈M表示相似，∈C表示不相似。我们希望相似样本之间的距离小，不相似样本之间的距离大。因此可以求解下面的凸优化问题获得适当的度量矩阵**M**：
![西瓜书-10.度量学习-凸优化问题.gif](/img/西瓜书-10.度量学习-凸优化问题.gif)

